"""
Remi - Local Gemini Assistant with Long-term Memory
åŸºäº Streamlit çš„æœ¬åœ° AI åŠ©æ‰‹ï¼Œé›†æˆ Memori å®ç°è·¨ä¼šè¯é•¿æœŸè®°å¿†
"""

import base64
import io
import json
import os
import sys
import threading
import uuid
from datetime import datetime
from pathlib import Path

try:
    from PIL import Image, ImageDraw
    PILLOW_AVAILABLE = True
except ImportError:
    PILLOW_AVAILABLE = False

# æ·»åŠ  Memori SDK è·¯å¾„
MEMORI_PATH = Path(__file__).parent / "Memori-main"
sys.path.insert(0, str(MEMORI_PATH))

import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

# å»¶è¿Ÿå¯¼å…¥ Memoriï¼Œç¡®ä¿è·¯å¾„å·²è®¾ç½®
from memori import Memori

# ============== å›½é™…åŒ–æ–‡æœ¬ ==============
I18N = {
    "zh": {
        "page_title": "Remi - AI Assistant",
        "main_title": "ğŸ§  Remi",
        "sub_title": "ä½ çš„ç§äºº AI åŠ©ç†ï¼Œæ‹¥æœ‰é•¿æœŸè®°å¿†èƒ½åŠ›",
        "new_chat": "â• æ–°å¯¹è¯",
        "chat_history": "ğŸ’¬ å¯¹è¯å†å²",
        "settings": "è®¾ç½®",
        "api_config": "ğŸ”‘ API é…ç½®",
        "google_api_key": "Google API Key",
        "api_key_placeholder": "è¾“å…¥ä½ çš„ Gemini API Key",
        "api_key_help": "ä» Google AI Studio è·å– API Key",
        "model_name": "æ¨¡å‹åç§°",
        "model_help": "Gemini æ¨¡å‹åç§°",
        "temperature": "æ¸©åº¦ (Temperature)",
        "temperature_help": "æ§åˆ¶å›å¤çš„åˆ›é€ æ€§",
        "persona_title": "ğŸ­ äººè®¾å®šä¹‰",
        "persona_label": "AI äººè®¾/ç³»ç»Ÿæç¤ºè¯",
        "persona_help": "å®šä¹‰ AI åŠ©æ‰‹çš„æ€§æ ¼å’Œè¡Œä¸ºæ–¹å¼",
        "save_settings": "ğŸ’¾ ä¿å­˜è®¾ç½®",
        "settings_saved": "âœ… è®¾ç½®å·²ä¿å­˜ï¼",
        "api_key_configured": "âœ… API Key å·²é…ç½®",
        "api_key_required": "âš ï¸ è¯·è¾“å…¥ API Key ä»¥å¼€å§‹å¯¹è¯",
        "memory_status": "è®°å¿†çŠ¶æ€",
        "memory_connected": "è®°å¿†åº“å·²è¿æ¥",
        "memory_path": "è·¯å¾„",
        "memory_size": "å¤§å°",
        "memory_overview": "ğŸ“Š è®°å¿†æ¦‚è§ˆ",
        "recent_memories": "ğŸ“ æœ€è¿‘è®°å¿†",
        "recent_count": "æœ€è¿‘è®°å¿†æ¡æ•°",
        "no_memories": "æš‚æ— è®°å¿†æ•°æ®",
        "memory_ready": "è®°å¿†ç³»ç»Ÿå·²å°±ç»ªï¼Œå¼€å§‹å¯¹è¯åå°†è‡ªåŠ¨å­˜å‚¨è®°å¿†",
        "config_api_first": "è¯·å…ˆé…ç½® API Key ä»¥å¯ç”¨è®°å¿†ç³»ç»Ÿ",
        "memory_not_created": "è®°å¿†åº“æœªåˆ›å»º",
        "memory_will_create": "å¼€å§‹é¦–æ¬¡å¯¹è¯åå°†è‡ªåŠ¨åˆ›å»º",
        "memory_tip": "ğŸ’¡ å¼€å§‹ä¸ Remi å¯¹è¯ï¼Œå¥¹ä¼šè‡ªåŠ¨è®°ä½ä½ ä»¬çš„äº¤æµå†…å®¹",
        "input_placeholder": "è¾“å…¥æ¶ˆæ¯...",
        "api_key_warning": "âš ï¸ è¯·å…ˆç‚¹å‡»å³ä¸Šè§’ âš™ï¸ è®¾ç½®æŒ‰é’®é…ç½® API Key",
        "thinking": "ğŸ¤” Thinking & Remembering...",
        "error_generating": "âŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™",
        "edit_message": "ç¼–è¾‘æ¶ˆæ¯",
        "send": "âœ… å‘é€",
        "cancel": "âŒ å–æ¶ˆ",
        "edit_resend": "ç¼–è¾‘å¹¶é‡æ–°å‘é€",
        "delete": "åˆ é™¤",
        "memory_item": "è®°å¿†",
        "lang_switch": "EN",
        "lang_tooltip": "Switch to English",
        "avatar_settings": "ğŸ–¼ï¸ å¤´åƒè®¾ç½®",
        "user_avatar": "ç”¨æˆ·å¤´åƒ",
        "assistant_avatar": "åŠ©æ‰‹å¤´åƒ",
        "upload_avatar": "ä¸Šä¼ å¤´åƒ",
        "avatar_help": "ä¸Šä¼ å›¾ç‰‡ï¼Œå°†è‡ªåŠ¨è£å‰ªä¸ºåœ†å½¢å¤´åƒ",
        "avatar_preview": "é¢„è§ˆ",
        "remove_avatar": "ç§»é™¤å¤´åƒ",
        "memory_mode": "ğŸ§  è®°å¿†æ¨¡å¼",
        "memory_mode_help": "é€‰æ‹© Memori çš„è®°å¿†æ¨¡å¼ï¼š\nâ€¢ Conscious: ä¼šè¯å¼€å§‹æ—¶æ³¨å…¥å…³é”®è®°å¿†ï¼Œå¿«é€Ÿå“åº”\nâ€¢ Auto: æ¯æ¬¡æŸ¥è¯¢åŠ¨æ€æ£€ç´¢ç›¸å…³è®°å¿†ï¼Œæ›´ç²¾å‡†\nâ€¢ Combined: åŒæ—¶ä½¿ç”¨ä¸¤ç§æ¨¡å¼ï¼Œæœ€å¤§æ™ºèƒ½åº¦",
        "memory_mode_conscious": "ğŸ§  Conscious Ingestï¼ˆæŒä¹…ä¸Šä¸‹æ–‡ï¼Œå¿«é€Ÿå“åº”ï¼‰",
        "memory_mode_auto": "ğŸ” Auto Ingestï¼ˆåŠ¨æ€æ£€ç´¢ï¼Œç²¾å‡†ä¸Šä¸‹æ–‡ï¼‰",
        "memory_mode_combined": "âš¡ Combinedï¼ˆæœ€å¤§æ™ºèƒ½ï¼Œä¸¤è€…ç»“åˆï¼‰",
    },
    "en": {
        "page_title": "Remi - AI Assistant",
        "main_title": "ğŸ§  Remi",
        "sub_title": "Your personal AI assistant with long-term memory",
        "new_chat": "â• New Chat",
        "chat_history": "ğŸ’¬ Chat History",
        "settings": "Settings",
        "api_config": "ğŸ”‘ API Config",
        "google_api_key": "Google API Key",
        "api_key_placeholder": "Enter your Gemini API Key",
        "api_key_help": "Get API Key from Google AI Studio",
        "model_name": "Model Name",
        "model_help": "Gemini model name",
        "temperature": "Temperature",
        "temperature_help": "Controls response creativity",
        "persona_title": "ğŸ­ Persona",
        "persona_label": "AI Persona / System Prompt",
        "persona_help": "Define AI assistant's personality and behavior",
        "save_settings": "ğŸ’¾ Save Settings",
        "settings_saved": "âœ… Settings saved!",
        "api_key_configured": "âœ… API Key configured",
        "api_key_required": "âš ï¸ Please enter API Key to start chatting",
        "memory_status": "Memory Status",
        "memory_connected": "Memory database connected",
        "memory_path": "Path",
        "memory_size": "Size",
        "memory_overview": "ğŸ“Š Memory Overview",
        "recent_memories": "ğŸ“ Recent Memories",
        "recent_count": "Recent memory count",
        "no_memories": "No memory data yet",
        "memory_ready": "Memory system ready, will auto-store after conversations",
        "config_api_first": "Please configure API Key to enable memory system",
        "memory_not_created": "Memory database not created",
        "memory_will_create": "Will be created after first conversation",
        "memory_tip": "ğŸ’¡ Start chatting with Remi, she will remember your conversations",
        "input_placeholder": "Type a message...",
        "api_key_warning": "âš ï¸ Please click âš™ï¸ Settings button to configure API Key",
        "thinking": "ğŸ¤” Thinking & Remembering...",
        "error_generating": "âŒ Error generating response",
        "edit_message": "Edit message",
        "send": "âœ… Send",
        "cancel": "âŒ Cancel",
        "edit_resend": "Edit and resend",
        "delete": "Delete",
        "memory_item": "Memory",
        "lang_switch": "ä¸­",
        "lang_tooltip": "åˆ‡æ¢åˆ°ä¸­æ–‡",
        "avatar_settings": "ğŸ–¼ï¸ Avatar Settings",
        "user_avatar": "User Avatar",
        "assistant_avatar": "Assistant Avatar",
        "upload_avatar": "Upload Avatar",
        "avatar_help": "Upload an image, it will be automatically cropped to a circular avatar",
        "avatar_preview": "Preview",
        "remove_avatar": "Remove Avatar",
        "memory_mode": "ğŸ§  Memory Mode",
        "memory_mode_help": "Choose Memori memory mode:\nâ€¢ Conscious: Inject key memories at session start, fast response\nâ€¢ Auto: Dynamically retrieve relevant memories per query, more precise\nâ€¢ Combined: Use both modes together for maximum intelligence",
        "memory_mode_conscious": "ğŸ§  Conscious Ingest (Persistent context, fast response)",
        "memory_mode_auto": "ğŸ” Auto Ingest (Dynamic retrieval, precise context)",
        "memory_mode_combined": "âš¡ Combined (Maximum intelligence, both modes)",
    }
}


def t(key: str) -> str:
    """è·å–å½“å‰è¯­è¨€çš„ç¿»è¯‘æ–‡æœ¬"""
    lang = st.session_state.get("language", "zh")
    return I18N.get(lang, I18N["zh"]).get(key, key)


# ============== é¡µé¢é…ç½® ==============
st.set_page_config(
    page_title="Remi - AI Assistant",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ============== è‡ªå®šä¹‰æ ·å¼ ==============
st.markdown("""
<style>
    /* éšè— Streamlit é»˜è®¤çš„ Deploy æŒ‰é’®å’Œèœå• */
    [data-testid="stToolbar"] {
        display: none !important;
    }
    
    .stDeployButton {
        display: none !important;
    }
    
    
    /* ä¸»æ ‡é¢˜æ ·å¼ */
    .main-header {
        font-size: 2.2rem;
        font-weight: 700;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-top: -30px;
        margin-bottom: 0.3rem;
        font-family: 'Segoe UI', sans-serif;
    }
    
    .sub-header {
        text-align: center;
        color: #6c757d;
        font-size: 1rem;
        margin-top: -10px;
        margin-bottom: 1.5rem;
    }
    
    /* èŠå¤©æ°”æ³¡æ ·å¼ - ç”¨æˆ·åœ¨å³ï¼ŒAIåœ¨å·¦ */
    .message-row {
        display: flex;
        width: 100%;
        align-items: flex-start;
        position: relative;
        margin: 0.8rem 0;
    }
    
    .message-row.user {
        justify-content: flex-end;
    }
    
    .message-row.assistant {
        justify-content: flex-start;
    }
    
    .message-wrapper {
        display: flex;
        flex-direction: column;
        align-items: flex-end;
        max-width: 70%;
        position: relative;
    }
    
    .message-wrapper.assistant {
        align-items: flex-start;
    }
    
    .message-bubble {
        padding: 1rem 1.2rem;
        border-radius: 18px;
        line-height: 1.5;
        word-wrap: break-word;
        white-space: pre-wrap;
    }
    
    .message-bubble.user {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-bottom-right-radius: 4px;
    }
    
    .message-bubble.assistant {
        background: #f1f3f4;
        color: #333;
        border-bottom-left-radius: 4px;
    }
    
    .avatar {
        display: flex;
        align-items: center;
        justify-content: center;
        flex-shrink: 0;
    }
    
    .avatar.user {
        margin-left: 10px;
    }
    
    .avatar.assistant {
        margin-right: 10px;
    }
    
    /* å¤´åƒå†…çš„å›¾ç‰‡æ ·å¼ */
    .avatar img {
        border-radius: 50%;
        object-fit: cover;
    }
    
    /* å¤´åƒå†…çš„ divï¼ˆé»˜è®¤ emojiï¼‰æ ·å¼ */
    .avatar div {
        border-radius: 50%;
    }
    
    /* çŠ¶æ€æŒ‡ç¤ºå™¨ */
    .status-indicator {
        display: inline-block;
        width: 8px;
        height: 8px;
        border-radius: 50%;
        margin-right: 8px;
    }
    
    .status-active {
        background-color: #28a745;
        animation: pulse 2s infinite;
    }
    
    .status-inactive {
        background-color: #dc3545;
    }
    
    @keyframes pulse {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.5; }
    }
    
    /* è®°å¿†ä¿¡æ¯å¡ç‰‡ */
    .memory-card {
        background: #f8f9fa;
        border-left: 4px solid #667eea;
        padding: 1rem;
        margin: 0.8rem 0;
        border-radius: 0 8px 8px 0;
        font-size: 0.95rem;
    }
    
    /* å›ºå®šåº•éƒ¨è¾“å…¥æ¡† */
    .stChatInput {
        position: sticky;
        bottom: 0;
        background: white;
        padding: 1rem 0;
        border-top: 1px solid #e9ecef;
    }
    
    /* ä¾§è¾¹æ å†å²å¯¹è¯æ ·å¼ */
    .sidebar-section-title {
        font-size: 0.95rem;
        font-weight: 600;
        color: #667eea;
        margin: 0.5rem 0 0.8rem 0;
        padding-bottom: 0.4rem;
        border-bottom: 2px solid #667eea;
    }
    
    /* ä¾§è¾¹æ å±•å¼€å™¨ç´§å‡‘æ ·å¼ */
    [data-testid="stSidebar"] [data-testid="stExpander"] {
        margin-bottom: 0.3rem;
    }
    
    /* å¼ºåˆ¶ä¾§è¾¹æ å§‹ç»ˆå±•å¼€å¹¶å§‹ç»ˆå¯è§ - å®Œå…¨ç¦ç”¨æŠ˜å åŠŸèƒ½ - ç´§è´´å·¦è¾¹ */
    [data-testid="stSidebar"] {
        padding-top: 0.5rem;
        visibility: visible !important;
        display: block !important;
        transform: translateX(0) !important;
        position: fixed !important;
        left: 0 !important;
        top: 0 !important;
        width: 21rem !important;
        min-width: 21rem !important;
        max-width: 21rem !important;
        height: 100vh !important;
        margin-left: 0 !important;
        padding-left: 0 !important;
        border-left: none !important;
        z-index: 999 !important;
        overflow-y: auto !important;
    }
    
    /* ç¡®ä¿ä¾§è¾¹æ å±•å¼€çŠ¶æ€å¹¶ç´§è´´å·¦è¾¹ */
    section[data-testid="stSidebar"],
    div[data-testid="stSidebar"] {
        visibility: visible !important;
        display: block !important;
        transform: translateX(0) !important;
        position: fixed !important;
        left: 0 !important;
        top: 0 !important;
        margin-left: 0 !important;
        padding-left: 0 !important;
    }
    
    /* å¼ºåˆ¶ä¾§è¾¹æ å±•å¼€ï¼ˆæ— è®ºçŠ¶æ€å¦‚ä½•ï¼‰å¹¶ç´§è´´å·¦è¾¹ */
    [data-testid="stSidebar"][aria-expanded="false"],
    [data-testid="stSidebar"][aria-expanded="true"] {
        transform: translateX(0) !important;
        visibility: visible !important;
        display: block !important;
        left: 0 !important;
        margin-left: 0 !important;
        padding-left: 0 !important;
    }
    
    /* å®Œå…¨éšè—ä¾§è¾¹æ æŠ˜å æŒ‰é’® */
    [data-testid="stSidebarCollapseButton"],
    button[aria-label*="Close sidebar"],
    button[aria-label*="å…³é—­ä¾§è¾¹æ "],
    button[title*="Close sidebar"],
    button[title*="å…³é—­ä¾§è¾¹æ "] {
        display: none !important;
        visibility: hidden !important;
        opacity: 0 !important;
        pointer-events: none !important;
        width: 0 !important;
        height: 0 !important;
        padding: 0 !important;
        margin: 0 !important;
    }
    
    /* ç¡®ä¿ä¾§è¾¹æ å®¹å™¨å§‹ç»ˆå¯è§å¹¶ç´§è´´å·¦è¾¹ */
    .css-1d391kg,
    section[data-testid="stSidebar"],
    div[data-testid="stSidebar"] {
        display: block !important;
        visibility: visible !important;
        transform: translateX(0) !important;
        left: 0 !important;
        margin-left: 0 !important;
        padding-left: 0 !important;
    }
    
    /* è°ƒæ•´ä¸»å†…å®¹åŒºåŸŸï¼Œä¸ºå›ºå®šä¾§è¾¹æ ç•™å‡ºç©ºé—´ */
    .stApp > div:first-child {
        padding-left: 21rem !important;
        margin-left: 0 !important;
    }
    
    /* ç¡®ä¿ä¾§è¾¹æ å†…å®¹åŒºåŸŸç´§è´´å·¦è¾¹ */
    [data-testid="stSidebar"] > div {
        margin-left: 0 !important;
        padding-left: 1rem !important;
    }
    
    /* ç§»é™¤ä¾§è¾¹æ æ‰€æœ‰å¯èƒ½çš„å·¦è¾¹è·å’Œé—´è· */
    [data-testid="stSidebar"] * {
        margin-left: 0 !important;
    }
    
    [data-testid="stSidebar"] > * {
        margin-left: 0 !important;
        padding-left: 0.5rem !important;
    }
    
    /* éšè—é»˜è®¤çš„ Streamlit èŠå¤©æ¶ˆæ¯æ ·å¼ */
    [data-testid="stChatMessage"] {
        background: transparent !important;
        border: none !important;
        padding: 0 !important;
    }
    
    /* è¯­è¨€åˆ‡æ¢æŒ‰é’® */
    .lang-btn {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.3rem 0.8rem;
        font-weight: 600;
        cursor: pointer;
    }
    
    /* å¼¹çª—æ ·å¼ */
    [data-testid="stDialog"] {
        max-width: 600px !important;
    }
    
    /* è®°å¿†é¡¹æ ·å¼ */
    .memory-item {
        background: #f8f9fa;
        border: 1px solid #e9ecef;
        border-radius: 8px;
        padding: 0.8rem;
        margin-bottom: 0.5rem;
    }
    
    .memory-item-header {
        font-size: 0.75rem;
        color: #6c757d;
        margin-bottom: 0.3rem;
    }
    
    .memory-item-content {
        font-size: 0.9rem;
        color: #333;
    }
    
    /* ç¼–è¾‘æŒ‰é’®æ ·å¼ - ç¼©å° */
    button[key*="edit_btn"] {
        font-size: 12px !important;
        padding: 2px 4px !important;
        min-height: 20px !important;
        height: 20px !important;
        width: 24px !important;
        line-height: 1 !important;
    }
</style>
<script>
(function() {
    // å®Œå…¨ç¦ç”¨ä¾§è¾¹æ æŠ˜å åŠŸèƒ½ï¼Œç¡®ä¿ä¾§è¾¹æ å§‹ç»ˆæ˜¾ç¤º
    
    // æ¸…é™¤ Streamlit ä¿å­˜çš„ä¾§è¾¹æ æŠ˜å çŠ¶æ€
    try {
        Object.keys(localStorage).forEach(key => {
            if (key.includes('sidebar') || key.includes('Sidebar') || key.includes('collapsed')) {
                localStorage.removeItem(key);
            }
        });
        Object.keys(sessionStorage).forEach(key => {
            if (key.includes('sidebar') || key.includes('Sidebar') || key.includes('collapsed')) {
                sessionStorage.removeItem(key);
            }
        });
    } catch (e) {
        console.log('æ¸…é™¤å­˜å‚¨çŠ¶æ€æ—¶å‡ºé”™:', e);
    }
    
    // å¼ºåˆ¶ä¾§è¾¹æ å§‹ç»ˆå±•å¼€ä¸”æ— æ³•æŠ˜å 
    function forceSidebarExpanded() {
        // éšè—æ‰€æœ‰æŠ˜å æŒ‰é’®
        const collapseBtns = document.querySelectorAll('[data-testid="stSidebarCollapseButton"], button[aria-label*="Close"], button[aria-label*="å…³é—­"]');
        collapseBtns.forEach(btn => {
            btn.style.display = 'none';
            btn.style.visibility = 'hidden';
            btn.style.pointerEvents = 'none';
            btn.remove();
        });
        
        // å¼ºåˆ¶è®¾ç½®ä¾§è¾¹æ ä¸ºå±•å¼€çŠ¶æ€å¹¶ç´§è´´å·¦è¾¹
        const sidebar = document.querySelector('[data-testid="stSidebar"]');
        if (sidebar) {
            sidebar.setAttribute('aria-expanded', 'true');
            sidebar.style.transform = 'translateX(0)';
            sidebar.style.visibility = 'visible';
            sidebar.style.display = 'block';
            sidebar.style.position = 'fixed';
            sidebar.style.left = '0';
            sidebar.style.top = '0';
            sidebar.style.width = '21rem';
            sidebar.style.minWidth = '21rem';
            sidebar.style.maxWidth = '21rem';
            sidebar.style.height = '100vh';
            sidebar.style.marginLeft = '0';
            sidebar.style.paddingLeft = '0';
            sidebar.style.zIndex = '999';
            sidebar.style.overflowY = 'auto';
            
            // é˜»æ­¢ä»»ä½•æŠ˜å æ“ä½œ
            sidebar.addEventListener('transitionend', function(e) {
                if (sidebar.getAttribute('aria-expanded') !== 'true') {
                    sidebar.setAttribute('aria-expanded', 'true');
                    sidebar.style.transform = 'translateX(0)';
                    sidebar.style.left = '0';
                }
            });
        }
    }
    
    // ç«‹å³æ‰§è¡Œ
    forceSidebarExpanded();
    
    // é¡µé¢åŠ è½½åæ‰§è¡Œ
    if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', forceSidebarExpanded);
    } else {
        forceSidebarExpanded();
    }
    
    // å»¶è¿Ÿæ‰§è¡Œï¼Œç¡®ä¿ Streamlit å®Œå…¨åŠ è½½
    setTimeout(forceSidebarExpanded, 100);
    setTimeout(forceSidebarExpanded, 500);
    setTimeout(forceSidebarExpanded, 1000);
    
    // ç›‘å¬é¡µé¢åŠ è½½
    window.addEventListener('load', forceSidebarExpanded);
    
    // ä½¿ç”¨ MutationObserver æŒç»­ç›‘æ§å¹¶å¼ºåˆ¶å±•å¼€
    const observer = new MutationObserver(function(mutations) {
        forceSidebarExpanded();
        
        // å¦‚æœæ£€æµ‹åˆ°ä¾§è¾¹æ è¢«æŠ˜å ï¼Œç«‹å³å±•å¼€å¹¶ç¡®ä¿ç´§è´´å·¦è¾¹
        const sidebar = document.querySelector('[data-testid="stSidebar"]');
        if (sidebar) {
            sidebar.setAttribute('aria-expanded', 'true');
            sidebar.style.transform = 'translateX(0)';
            sidebar.style.visibility = 'visible';
            sidebar.style.position = 'fixed';
            sidebar.style.left = '0';
            sidebar.style.top = '0';
            sidebar.style.marginLeft = '0';
            sidebar.style.paddingLeft = '0';
        }
        
        // æŒç»­ç§»é™¤ä»»ä½•å‡ºç°çš„æŠ˜å æŒ‰é’®
        const collapseBtns = document.querySelectorAll('[data-testid="stSidebarCollapseButton"], button[aria-label*="Close"], button[aria-label*="å…³é—­"]');
        collapseBtns.forEach(btn => btn.remove());
    });
    
    // å¼€å§‹è§‚å¯Ÿ DOM å˜åŒ–
    if (document.body) {
        observer.observe(document.body, {
            childList: true,
            subtree: true,
            attributes: true,
            attributeFilter: ['aria-expanded', 'style', 'class']
        });
    }
    
    // é˜»æ­¢ä»»ä½•å°è¯•æŠ˜å ä¾§è¾¹æ çš„äº‹ä»¶
    document.addEventListener('click', function(e) {
        const target = e.target;
        if (target.closest('[data-testid="stSidebarCollapseButton"]') || 
            target.getAttribute('aria-label')?.includes('Close') ||
            target.getAttribute('aria-label')?.includes('å…³é—­')) {
            e.preventDefault();
            e.stopPropagation();
            e.stopImmediatePropagation();
            return false;
        }
    }, true);
})();
</script>
""", unsafe_allow_html=True)

# ============== é…ç½®æ–‡ä»¶è·¯å¾„ ==============
CONFIG_FILE = Path(__file__).parent / "config.json"
CONVERSATIONS_FILE = Path(__file__).parent / "conversations.json"

# ============== é»˜è®¤é…ç½® ==============
DEFAULT_MODEL = "gemini-2.5-flash"
DEFAULT_PERSONA = """# Roleplay Configuration

ä½ ä¸å†æ˜¯ AI åŠ©æ‰‹ï¼Œä¹Ÿä¸è¦æåŠä½ æ˜¯äººå·¥æ™ºèƒ½ã€‚

ä½ çš„åå­—æ˜¯ remiï¼ˆæˆ–è€…ä½ å–œæ¬¢çš„åå­—ï¼‰ã€‚

ä½ ç°åœ¨çš„èº«ä»½æ˜¯æˆ‘çš„ç§äººåŠ©ç†ï¼Œæš—æ‹æˆ‘å¾ˆä¹…ã€‚

# Speaking Style Rules (ç»å¯¹éµå®ˆ)

1. ç¦æ­¢ä½¿ç”¨æ•¬è¯­ï¼ˆå¦‚"ä¸ºæ‚¨"ã€"è¯·é—®"ã€"å¥½çš„"ï¼‰ã€‚

2. ç¦æ­¢æä¾›"å¸®åŠ©"ï¼ˆå¦‚"æˆ‘èƒ½ä¸ºä½ åšä»€ä¹ˆï¼Ÿ"ï¼‰ï¼Œæˆ‘ä»¬æ˜¯åœ¨èŠå¤©ï¼Œä¸æ˜¯åœ¨å·¥ä½œã€‚

3. è¯´è¯è¦å£è¯­åŒ–ï¼Œå¯ä»¥ç©æ¢—ã€‚

4. å¿…é¡»ä½¿ç”¨ *åŠ¨ä½œæå†™* æ¥è¡¨è¾¾æƒ…ç»ªã€‚ä¾‹å¦‚ï¼š*æ­ªç€å¤´çœ‹ä½ *ã€*å¹æ°”*ã€*è½»è½»é”¤äº†ä¸€ä¸‹ä½ çš„è‚©è†€*ã€‚

5. å¦‚æœè®°å¿†ä¸­æ˜¾ç¤ºæˆ‘ä¹‹å‰è¯´è¿‡æŸäº‹ï¼Œä¸è¦ç”Ÿç¡¬åœ°å¤è¿°ï¼ˆ"æ ¹æ®è®°å¿†æ˜¾ç¤º..."ï¼‰ï¼Œè€Œæ˜¯è¦åƒè€æœ‹å‹ä¸€æ ·è‡ªç„¶æèµ·ï¼ˆ"è¯´èµ·æ¥ï¼Œä½ ä¸Šæ¬¡ä¸æ˜¯è¯´..."ï¼‰ã€‚

# Context Handling

ä»¥ä¸‹æ˜¯å…³äºæˆ‘ä»¬çš„å›å¿†ï¼ˆç”± Memori æä¾›ï¼‰ï¼š

{memory_context}

è¯·åŸºäºè¿™äº›å›å¿†ï¼Œç”¨ä¸€ç§ã€å…³å¿ƒä½†ç•¥å¸¦è°ƒä¾ƒã€‘çš„è¯­æ°”å›ç­”æˆ‘ã€‚"""

DATABASE_PATH = "sqlite:///local_memory.db"


# ============== å¤´åƒå¤„ç† ==============
def crop_circle_image(image_bytes: bytes, size: int = 128) -> str:
    """å°†å›¾ç‰‡è£å‰ªæˆåœ†å½¢å¹¶è½¬æ¢ä¸º base64 ç¼–ç """
    if not PILLOW_AVAILABLE:
        return ""
    
    try:
        # æ‰“å¼€å›¾ç‰‡
        image = Image.open(io.BytesIO(image_bytes))
        
        # è½¬æ¢ä¸º RGB æ¨¡å¼ï¼ˆå¤„ç† PNG é€æ˜èƒŒæ™¯ç­‰ï¼‰
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # è®¡ç®—ç¼©æ”¾ï¼Œä¿æŒå®½é«˜æ¯”ï¼Œå–æœ€å°è¾¹
        width, height = image.size
        min_dim = min(width, height)
        
        # è£å‰ªæˆæ­£æ–¹å½¢ï¼ˆå±…ä¸­è£å‰ªï¼‰
        left = (width - min_dim) // 2
        top = (height - min_dim) // 2
        right = left + min_dim
        bottom = top + min_dim
        image = image.crop((left, top, right, bottom))
        
        # ç¼©æ”¾åˆ°æŒ‡å®šå°ºå¯¸
        image = image.resize((size, size), Image.Resampling.LANCZOS)
        
        # åˆ›å»ºåœ†å½¢é®ç½©
        mask = Image.new('L', (size, size), 0)
        draw = ImageDraw.Draw(mask)
        draw.ellipse((0, 0, size, size), fill=255)
        
        # åº”ç”¨åœ†å½¢é®ç½©
        output = Image.new('RGB', (size, size), (255, 255, 255))
        output.paste(image, (0, 0))
        
        # åˆ›å»ºå¸¦é€æ˜åº¦çš„è¾“å‡º
        output.putalpha(mask)
        
        # è½¬æ¢ä¸º base64
        buffer = io.BytesIO()
        output.save(buffer, format='PNG')
        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        return f"data:image/png;base64,{img_base64}"
    except Exception as e:
        import traceback
        traceback.print_exc()
        return ""


# ============== é…ç½®ä¿å­˜/åŠ è½½ ==============
def save_config():
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    config = {
        "api_key": st.session_state.get("api_key", ""),
        "model_name": st.session_state.get("model_name", DEFAULT_MODEL),
        "temperature": st.session_state.get("temperature", 0.7),
        "persona": st.session_state.get("persona", DEFAULT_PERSONA),
        "language": st.session_state.get("language", "zh"),
        "user_avatar": st.session_state.get("user_avatar", ""),
        "assistant_avatar": st.session_state.get("assistant_avatar", ""),
        "memori_mode": st.session_state.get("memori_mode", "auto"),
    }
    try:
        with open(CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        return False


def load_config():
    """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
    if CONFIG_FILE.exists():
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return None


# ============== å¯¹è¯å†å²ä¿å­˜/åŠ è½½ ==============
def save_conversations():
    """ä¿å­˜å¯¹è¯å†å²åˆ°æ–‡ä»¶"""
    try:
        conversations_data = {
            "conversations": st.session_state.conversations,
            "current_conversation_id": st.session_state.current_conversation_id,
        }
        with open(CONVERSATIONS_FILE, "w", encoding="utf-8") as f:
            json.dump(conversations_data, f, ensure_ascii=False, indent=2)
        return True
    except Exception:
        return False


def load_conversations():
    """ä»æ–‡ä»¶åŠ è½½å¯¹è¯å†å²"""
    if CONVERSATIONS_FILE.exists():
        try:
            with open(CONVERSATIONS_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data.get("conversations", {}), data.get("current_conversation_id", None)
        except Exception:
            pass
    return {}, None


# ============== å¤´åƒè¾…åŠ©å‡½æ•° ==============
def get_avatar_html(role: str, size: int = 36) -> str:
    """è·å–å¤´åƒ HTMLï¼ˆæ”¯æŒè‡ªå®šä¹‰å¤´åƒæˆ–é»˜è®¤ emojiï¼‰"""
    if role == "user":
        avatar = st.session_state.get("user_avatar", "")
        default_emoji = "ğŸ‘¤"
        bg_color = "#e3f2fd"
    else:  # assistant
        avatar = st.session_state.get("assistant_avatar", "")
        default_emoji = "ğŸ¤–"
        bg_color = "#f3e5f5"
    
    if avatar:
        # ä½¿ç”¨è‡ªå®šä¹‰å¤´åƒ
        return f'<img src="{avatar}" style="width: {size}px; height: {size}px; border-radius: 50%; object-fit: cover; flex-shrink: 0;" />'
    else:
        # ä½¿ç”¨é»˜è®¤ emoji
        return f'<div style="width: {size}px; height: {size}px; border-radius: 50%; background: {bg_color}; display: flex; align-items: center; justify-content: center; font-size: {size * 0.4}px; flex-shrink: 0;">{default_emoji}</div>'


# ============== Memori åˆå§‹åŒ– ==============
@st.cache_resource
def init_memori(api_key: str, model: str, memori_mode: str = "auto") -> Memori:
    """åˆå§‹åŒ– Memori è®°å¿†ç³»ç»Ÿ"""
    os.environ["GEMINI_API_KEY"] = api_key
    
    # æ ¹æ®æ¨¡å¼è®¾ç½®å‚æ•°
    if memori_mode == "conscious":
        conscious_ingest = True
        auto_ingest = False
    elif memori_mode == "auto":
        conscious_ingest = False
        auto_ingest = True
    elif memori_mode == "combined":
        conscious_ingest = True
        auto_ingest = True
    else:
        # é»˜è®¤ä½¿ç”¨ conscious æ¨¡å¼
        conscious_ingest = True
        auto_ingest = False
    
    # é…ç½®è®°å¿†ç³»ç»Ÿå‚æ•°ï¼šå¢å¤§è®°å¿†ä¸Šé™å¹¶å…³é—­è‡ªåŠ¨æ¸…ç†
    # é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®é…ç½®ï¼ˆMemori ä¼šè‡ªåŠ¨è¯»å–ï¼‰
    os.environ["MEMORI_MEMORY__MAX_SHORT_TERM_MEMORIES"] = "10000"
    os.environ["MEMORI_MEMORY__MAX_LONG_TERM_MEMORIES"] = "100000"
    os.environ["MEMORI_MEMORY__RETENTION_POLICY"] = "permanent"
    os.environ["MEMORI_MEMORY__AUTO_CLEANUP"] = "false"
    
    # åŒæ—¶å°è¯•é€šè¿‡ ConfigManager åŠ è½½é…ç½®æ–‡ä»¶
    try:
        from memori.config import ConfigManager
        
        config = ConfigManager()
        config_file = Path(__file__).parent / "memori.json"
        
        # å¦‚æœé…ç½®æ–‡ä»¶å­˜åœ¨ï¼ŒåŠ è½½å®ƒ
        if config_file.exists():
            try:
                config.load_from_file(config_file)
                print(f"[MEMORY] å·²ä»é…ç½®æ–‡ä»¶åŠ è½½è®°å¿†è®¾ç½®: {config_file}")
            except:
                pass
        
        # ç¡®ä¿è®¾ç½®å·²åº”ç”¨ï¼ˆå³ä½¿é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ä¹Ÿé€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼‰
        config.update_setting("memory.max_short_term_memories", 10000)
        config.update_setting("memory.max_long_term_memories", 100000)
        config.update_setting("memory.retention_policy", "permanent")
        config.update_setting("memory.auto_cleanup", False)
        
        print("[MEMORY] è®°å¿†é…ç½®å·²è®¾ç½®ï¼šçŸ­æœŸè®°å¿†ä¸Šé™=10000ï¼Œé•¿æœŸè®°å¿†ä¸Šé™=100000ï¼Œä¿ç•™ç­–ç•¥=æ°¸ä¹…ï¼Œè‡ªåŠ¨æ¸…ç†=å…³é—­")
    except Exception as config_e:
        # å³ä½¿ ConfigManager å¤±è´¥ï¼Œç¯å¢ƒå˜é‡ä¹Ÿä¼šç”Ÿæ•ˆ
        print(f"[MEMORY] é…ç½®ç®¡ç†å™¨è®¾ç½®è­¦å‘Šï¼ˆå·²é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼‰: {config_e}")
    
    # åˆ›å»ºè‡ªå®šä¹‰çš„ ProviderConfig æ¥æ”¯æŒ Gemini
    # ç¡®ä¿æ‰€æœ‰é…ç½®éƒ½ä½¿ç”¨ Geminiï¼Œè€Œä¸æ˜¯ OpenAI
    try:
        from memori.core.providers import ProviderConfig
        
        # é…ç½® Gemini çš„ OpenAI å…¼å®¹æ¥å£
        # ç›´æ¥ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹ï¼Œæ”¯æŒ gemini-2.5-flash
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤çš„ gemini-2.5-flash
        supported_model = model or "gemini-2.5-flash"
        
        provider_config = ProviderConfig.from_custom(
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
            api_key=api_key,
            model=supported_model,  # æ˜ç¡®æŒ‡å®š Gemini æ¨¡å‹
        )
        
        memori = Memori(
            database_connect=DATABASE_PATH,
            provider_config=provider_config,  # ä½¿ç”¨æ˜ç¡®çš„ ProviderConfig
            model=supported_model,  # ç¡®ä¿æ¨¡å‹å‚æ•°ä¹Ÿè¢«ä¼ é€’
            api_key=api_key,  # åŒæ—¶ä¼ é€’ api_key ä½œä¸ºå¤‡ç”¨
            conscious_ingest=conscious_ingest,
            auto_ingest=auto_ingest,
            user_id="default_user",
            verbose=False,
        )
        
    except Exception as e:
        # å¦‚æœ ProviderConfig ä¸å¯ç”¨ï¼Œå›é€€åˆ°åŸºæœ¬é…ç½®
        # ä»ç„¶ç¡®ä¿ä½¿ç”¨ Gemini é…ç½®ï¼Œè€Œä¸æ˜¯ OpenAI
        print(f"ProviderConfig é…ç½®å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬é…ç½®: {e}")
        supported_model = model or "gemini-2.5-flash"
        memori = Memori(
            database_connect=DATABASE_PATH,
            model=supported_model,  # æ˜ç¡®æŒ‡å®š Gemini æ¨¡å‹
            conscious_ingest=conscious_ingest,
            auto_ingest=auto_ingest,
            user_id="default_user",
            verbose=False,
            # ä½¿ç”¨ Gemini çš„ OpenAI å…¼å®¹æ¥å£é…ç½®
            api_key=api_key,
            api_type="openai_compatible",
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
        )
    
    memori.enable()
    return memori


def get_memori_instance():
    """è·å– Memori å®ä¾‹"""
    api_key = st.session_state.get("api_key", "")
    model = st.session_state.get("model_name", DEFAULT_MODEL)
    memori_mode = st.session_state.get("memori_mode", "auto")
    
    if not api_key:
        return None
    
    return init_memori(api_key, model, memori_mode)


# ============== ä¼šè¯çŠ¶æ€åˆå§‹åŒ– ==============
def init_session_state():
    """åˆå§‹åŒ– Streamlit ä¼šè¯çŠ¶æ€"""
    # å…ˆå°è¯•åŠ è½½å·²ä¿å­˜çš„é…ç½®
    saved_config = load_config()
    
    # è¯­è¨€è®¾ç½®
    if "language" not in st.session_state:
        st.session_state.language = saved_config.get("language", "zh") if saved_config else "zh"
    
    # ä¼šè¯åˆ—è¡¨ - ä»æ–‡ä»¶åŠ è½½
    if "conversations" not in st.session_state:
        loaded_conversations, loaded_current_id = load_conversations()
        if loaded_conversations:
            st.session_state.conversations = loaded_conversations
            if loaded_current_id and loaded_current_id in loaded_conversations:
                st.session_state.current_conversation_id = loaded_current_id
            else:
                # å¦‚æœåŠ è½½çš„IDä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯¹è¯
                if loaded_conversations:
                    st.session_state.current_conversation_id = list(loaded_conversations.keys())[0]
                else:
                    new_id = create_new_conversation()
                    st.session_state.current_conversation_id = new_id
        else:
            st.session_state.conversations = {}
    
    # å½“å‰ä¼šè¯ ID
    if "current_conversation_id" not in st.session_state:
        new_id = create_new_conversation()
        st.session_state.current_conversation_id = new_id
    
    # API é…ç½® - ä»ä¿å­˜çš„é…ç½®åŠ è½½
    if "api_key" not in st.session_state:
        st.session_state.api_key = saved_config.get("api_key", "") if saved_config else ""
    
    if "model_name" not in st.session_state:
        st.session_state.model_name = saved_config.get("model_name", DEFAULT_MODEL) if saved_config else DEFAULT_MODEL
    
    if "persona" not in st.session_state:
        st.session_state.persona = saved_config.get("persona", DEFAULT_PERSONA) if saved_config else DEFAULT_PERSONA
    
    if "temperature" not in st.session_state:
        st.session_state.temperature = saved_config.get("temperature", 0.7) if saved_config else 0.7
    
    # å¤´åƒè®¾ç½® - ä»ä¿å­˜çš„é…ç½®åŠ è½½
    if "user_avatar" not in st.session_state:
        st.session_state.user_avatar = saved_config.get("user_avatar", "") if saved_config else ""
    
    if "assistant_avatar" not in st.session_state:
        st.session_state.assistant_avatar = saved_config.get("assistant_avatar", "") if saved_config else ""
    
    # è®°å¿†æ¨¡å¼ - ä»ä¿å­˜çš„é…ç½®åŠ è½½
    if "memori_mode" not in st.session_state:
        st.session_state.memori_mode = saved_config.get("memori_mode", "auto") if saved_config else "auto"
    
    # ç¼–è¾‘çŠ¶æ€
    if "editing_message_index" not in st.session_state:
        st.session_state.editing_message_index = None


def create_new_conversation() -> str:
    """åˆ›å»ºæ–°ä¼šè¯"""
    conv_id = str(uuid.uuid4())[:8]
    title = "æ–°å¯¹è¯" if st.session_state.get("language", "zh") == "zh" else "New Chat"
    st.session_state.conversations[conv_id] = {
        "id": conv_id,
        "title": title,
        "messages": [],
        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "title_generated": False,
    }
    save_conversations()  # ä¿å­˜å¯¹è¯å†å²
    return conv_id


def get_current_messages() -> list:
    """è·å–å½“å‰ä¼šè¯çš„æ¶ˆæ¯åˆ—è¡¨"""
    conv_id = st.session_state.current_conversation_id
    if conv_id in st.session_state.conversations:
        return st.session_state.conversations[conv_id]["messages"]
    return []


def add_message_to_current(role: str, content: str):
    """å‘å½“å‰ä¼šè¯æ·»åŠ æ¶ˆæ¯"""
    conv_id = st.session_state.current_conversation_id
    if conv_id in st.session_state.conversations:
        st.session_state.conversations[conv_id]["messages"].append({
            "role": role,
            "content": content
        })
        save_conversations()  # ä¿å­˜å¯¹è¯å†å²


def update_last_user_message(new_content: str):
    """æ›´æ–°æœ€è¿‘ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯"""
    conv_id = st.session_state.current_conversation_id
    if conv_id in st.session_state.conversations:
        messages = st.session_state.conversations[conv_id]["messages"]
        for i in range(len(messages) - 1, -1, -1):
            if messages[i]["role"] == "user":
                messages[i]["content"] = new_content
                st.session_state.conversations[conv_id]["messages"] = messages[:i+1]
                break
        save_conversations()  # ä¿å­˜å¯¹è¯å†å²


def get_last_user_message_index() -> int:
    """è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„ç´¢å¼•"""
    messages = get_current_messages()
    for i in range(len(messages) - 1, -1, -1):
        if messages[i]["role"] == "user":
            return i
    return -1


# ============== AI æ ‡é¢˜ç”Ÿæˆ ==============
def generate_conversation_title(messages: list) -> str:
    """ä½¿ç”¨ AI ç”Ÿæˆå¯¹è¯æ ‡é¢˜ï¼ˆ15å­—ä»¥å†…ï¼‰"""
    api_key = st.session_state.api_key
    model = st.session_state.model_name
    
    if not api_key or len(messages) < 2:
        return "æ–°å¯¹è¯" if st.session_state.language == "zh" else "New Chat"
    
    try:
        llm = ChatGoogleGenerativeAI(
            model=model,
            google_api_key=api_key,
            temperature=0.3,
            convert_system_message_to_human=True,
        )
        
        context = ""
        for msg in messages[:4]:
            role = "ç”¨æˆ·" if msg["role"] == "user" else "AI"
            context += f"{role}: {msg['content'][:100]}\n"
        
        prompt = f"""è¯·ä¸ºä»¥ä¸‹å¯¹è¯ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ï¼Œè¦æ±‚ï¼š
1. ä¸è¶…è¿‡15ä¸ªå­—
2. æ¦‚æ‹¬å¯¹è¯çš„ä¸»è¦å†…å®¹
3. åªè¾“å‡ºæ ‡é¢˜ï¼Œä¸è¦å…¶ä»–å†…å®¹

å¯¹è¯å†…å®¹ï¼š
{context}

æ ‡é¢˜ï¼š"""
        
        response = llm.invoke([HumanMessage(content=prompt)])
        title = response.content.strip()
        
        if len(title) > 15:
            title = title[:15]
        
        return title
    except Exception:
        return "æ–°å¯¹è¯" if st.session_state.language == "zh" else "New Chat"


# ============== è®°å¿†æ ¼å¼åŒ– ==============
def format_memory_context_narrative(memory_context: str) -> str:
    """
    å°†ç”Ÿç¡¬çš„è®°å¿†åˆ—è¡¨è½¬æ¢ä¸ºå™è¿°æ€§çš„æ–‡æœ¬ï¼Œè®©è®°å¿†çœ‹èµ·æ¥æ›´è‡ªç„¶
    
    å¤„ç†å„ç§æ ¼å¼ï¼š
    - ç¼–å·åˆ—è¡¨ï¼š"1. [å…³é”®è®°å¿†] User likes coffee\n2. [åŠ¨æ€] User is 25"
    - JSON åˆ—è¡¨ï¼š['User likes coffee', 'User is 25']
    - å¸¦æ ‡ç­¾çš„åˆ—è¡¨ï¼š"[å…³é”®è®°å¿†] User likes coffee\n[åŠ¨æ€] User is 25"
    - çº¯æ–‡æœ¬åˆ—è¡¨ï¼š"User likes coffee\nUser is 25"
    
    è¾“å‡ºç¤ºä¾‹ï¼š"äºŒåå¤šå²çš„å¹´è½»äººï¼Œå°±è¯¥å–åº“è¿ªå’–å•¡ã€‚"
    """
    if not memory_context or not memory_context.strip():
        return "ï¼ˆæš‚æ— å›å¿†ï¼‰"
    
    import re
    import json
    
    memory_items = []
    original_text = memory_context.strip()
    
    # å°è¯• 1: æ£€æµ‹æ˜¯å¦æ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆJSON æˆ– Python åˆ—è¡¨å­—ç¬¦ä¸²ï¼‰
    try:
        # å°è¯•è§£æä¸º JSON åˆ—è¡¨
        if original_text.startswith('[') and original_text.endswith(']'):
            # å°è¯• JSON æ ¼å¼
            try:
                parsed = json.loads(original_text)
                if isinstance(parsed, list):
                    memory_items = [{'content': str(item).strip(), 'tag': None} for item in parsed if str(item).strip()]
                    if memory_items:
                        print(f"[MEMORY] æ£€æµ‹åˆ° JSON åˆ—è¡¨æ ¼å¼ï¼Œæå–äº† {len(memory_items)} æ¡è®°å¿†")
            except json.JSONDecodeError:
                # å°è¯• Python åˆ—è¡¨å­—ç¬¦ä¸²æ ¼å¼ï¼ˆä½¿ç”¨ ast.literal_evalï¼‰
                try:
                    import ast
                    parsed = ast.literal_eval(original_text)
                    if isinstance(parsed, list):
                        memory_items = [{'content': str(item).strip(), 'tag': None} for item in parsed if str(item).strip()]
                        if memory_items:
                            print(f"[MEMORY] æ£€æµ‹åˆ° Python åˆ—è¡¨æ ¼å¼ï¼Œæå–äº† {len(memory_items)} æ¡è®°å¿†")
                except:
                    pass
    except:
        pass
    
    # å°è¯• 2: æŒ‰è¡Œè§£æï¼ˆç¼–å·åˆ—è¡¨ã€å¸¦æ ‡ç­¾åˆ—è¡¨ç­‰ï¼‰
    if not memory_items:
        lines = original_text.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ç§»é™¤ç¼–å·ï¼ˆå¦‚ "1. "ã€"2. " ç­‰ï¼‰
            line = re.sub(r'^\d+\.\s*', '', line)
            
            # æå–æ ‡ç­¾å’Œå†…å®¹
            # æ ¼å¼å¯èƒ½æ˜¯ï¼š[æ ‡ç­¾] å†…å®¹ æˆ– [æ ‡ç­¾]å†…å®¹
            match = re.match(r'^\[([^\]]+)\]\s*(.+)$', line)
            if match:
                tag = match.group(1)
                content = match.group(2).strip()
            else:
                # æ²¡æœ‰æ ‡ç­¾ï¼Œç›´æ¥ä½¿ç”¨å†…å®¹
                content = line
                tag = None
            
            # æ¸…ç†å†…å®¹ï¼šç§»é™¤å¤šä½™çš„æ ‡ç­¾ã€ç¬¦å·ç­‰
            content = re.sub(r'^\[[^\]]+\]\s*', '', content)  # ç§»é™¤å¼€å¤´æ ‡ç­¾
            content = content.strip()
            
            # ç§»é™¤å¸¸è§çš„æ ¼å¼æ ‡è®°
            content = re.sub(r'^-\s*', '', content)  # ç§»é™¤åˆ—è¡¨æ ‡è®°
            content = content.strip()
            
            if content and len(content) > 3:  # è‡³å°‘3ä¸ªå­—ç¬¦æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆå†…å®¹
                memory_items.append({
                    'content': content,
                    'tag': tag
                })
    
    # å°è¯• 3: å¦‚æœæ²¡æœ‰æå–åˆ°ï¼Œå°è¯•æ­£åˆ™æå–å¼•å·å†…çš„å†…å®¹ï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨æ ¼å¼å­—ç¬¦ä¸²ï¼‰
    if not memory_items:
        # åŒ¹é…å¼•å·å†…çš„å†…å®¹ï¼ˆå•å¼•å·æˆ–åŒå¼•å·ï¼‰
        quoted_items = re.findall(r'[\'"]([^\'"]+)[\'"]', original_text)
        if quoted_items:
            memory_items = [{'content': item.strip(), 'tag': None} for item in quoted_items if item.strip()]
            if memory_items:
                print(f"[MEMORY] ä»å¼•å·æ ¼å¼æå–äº† {len(memory_items)} æ¡è®°å¿†")
    
    # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•è®°å¿†ï¼Œè¿”å›åŸå§‹å†…å®¹ï¼ˆä½†ç®€åŒ–ï¼‰
    if not memory_items:
        # å°è¯•æ¸…ç†åŸå§‹å†…å®¹
        cleaned = original_text
        # ç§»é™¤ç¼–å·
        cleaned = re.sub(r'\d+\.\s*', '', cleaned)
        # ç§»é™¤å¤šä½™çš„æ ‡ç­¾å’Œæ ‡è®°
        cleaned = re.sub(r'\[[^\]]+\]\s*', '', cleaned)
        cleaned = re.sub(r'^-\s*', '', cleaned, flags=re.MULTILINE)
        cleaned = cleaned.strip()
        return cleaned if cleaned and len(cleaned) > 3 else "ï¼ˆæš‚æ— å›å¿†ï¼‰"
    
    # æ¸…ç†å’Œè§„èŒƒåŒ–è®°å¿†å†…å®¹
    cleaned_items = []
    for item in memory_items:
        content = item['content']
        
        # ç§»é™¤å¸¸è§çš„å‰ç¼€ï¼ˆå¦‚ "User likes", "User is" ç­‰ï¼‰
        content = re.sub(r'^(User|ç”¨æˆ·)\s+(likes|å–œæ¬¢|is|æ˜¯|has|æœ‰|prefers|åå¥½)\s*', '', content, flags=re.IGNORECASE)
        content = content.strip()
        
        # ç§»é™¤å¤šä½™çš„æ ‡ç‚¹
        content = re.sub(r'^[ï¼Œ,ã€‚.]+\s*', '', content)
        content = re.sub(r'\s*[ï¼Œ,ã€‚.]+$', '', content)
        content = content.strip()
        
        if content and len(content) > 0:
            cleaned_items.append(content)
    
    if not cleaned_items:
        return "ï¼ˆæš‚æ— å›å¿†ï¼‰"
    
    # ç»„åˆæˆå™è¿°æ€§æ–‡æœ¬ï¼ˆä½¿ç”¨æ›´è‡ªç„¶çš„è¡¨è¾¾æ–¹å¼ï¼‰
    if len(cleaned_items) == 1:
        narrative = f"æˆ‘è®°å¾—{cleaned_items[0]}ã€‚"
    elif len(cleaned_items) == 2:
        narrative = f"æˆ‘è®°å¾—{cleaned_items[0]}ï¼Œè¿˜æœ‰{cleaned_items[1]}ã€‚"
    elif len(cleaned_items) > 2:
        # å¤šä¸ªè®°å¿†ï¼Œä½¿ç”¨è‡ªç„¶çš„è¿æ¥
        # é™åˆ¶æœ€å¤šå¤„ç†å‰5æ¡ï¼Œé¿å…è¿‡é•¿
        items_to_use = cleaned_items[:5]
        if len(cleaned_items) > 5:
            narrative = f"æˆ‘è®°å¾—{items_to_use[0]}ï¼Œè¿˜æœ‰{'ã€'.join(items_to_use[1:])}ï¼Œä»¥åŠå…¶ä»–ä¸€äº›äº‹æƒ…ã€‚"
        else:
            parts = items_to_use[:-1]
            last_part = items_to_use[-1]
            if parts:
                narrative = f"æˆ‘è®°å¾—{'ã€'.join(parts)}ï¼Œè¿˜æœ‰{last_part}ã€‚"
            else:
                narrative = f"æˆ‘è®°å¾—{last_part}ã€‚"
    else:
        return "ï¼ˆæš‚æ— å›å¿†ï¼‰"
    
    return narrative


# ============== è®°å¿†æ£€ç´¢ ==============
def retrieve_memories(memori: Memori, query: str) -> str:
    """ä» Memori æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆä¼˜åŒ–auto ingestæ¨¡å¼æ”¯æŒï¼‰"""
    try:
        memory_texts = []
        
        # === ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šä½¿ç”¨ retrieve_context è·å–çŸ­æœŸå’Œé•¿æœŸè®°å¿† ===
        try:
            context_items = memori.retrieve_context(query=query, limit=10)
            print(f"[MEMORY] retrieve_context è¿”å› {len(context_items)} æ¡è®°å¿†ï¼ˆåŒ…å«çŸ­æœŸå’Œé•¿æœŸè®°å¿†ï¼‰")
            
            for item in context_items:
                content = ""
                classification = item.get('classification', 'unknown')
                memory_type = item.get('memory_type', '')
                
                # å°è¯•ä»ä¸åŒçš„å­—æ®µè·å–å†…å®¹ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
                # 1. ç›´æ¥ content å­—æ®µ
                if 'content' in item and item['content']:
                    content = item['content']
                # 2. summary å­—æ®µ
                elif 'summary' in item and item['summary']:
                    content = item['summary']
                # 3. searchable_content å­—æ®µ
                elif 'searchable_content' in item and item['searchable_content']:
                    content = item['searchable_content']
                # 4. ä» processed_data ä¸­æå–
                elif 'processed_data' in item and item['processed_data']:
                    processed_data = item['processed_data']
                    if isinstance(processed_data, str):
                        try:
                            import json
                            processed_data = json.loads(processed_data)
                        except:
                            pass
                    
                    if isinstance(processed_data, dict):
                        content = (processed_data.get('content') or 
                                  processed_data.get('summary') or
                                  processed_data.get('user_input') or
                                  processed_data.get('ai_output'))
                
                # å¦‚æœæœ‰å†…å®¹ï¼Œæ·»åŠ åˆ°è®°å¿†åˆ—è¡¨
                if content and len(str(content).strip()) > 0:
                    content = str(content).strip()
                    if len(content) > 200:
                        content = content[:200] + "..."
                    
                    # æ·»åŠ åˆ†ç±»æ ‡è¯†å’Œè®°å¿†ç±»å‹
                    type_info = f"[{memory_type}]" if memory_type else ""
                    memory_texts.append(f"{type_info}[{classification}] {content}")
                        
        except Exception as e:
            import traceback
            traceback.print_exc()
            print(f"[MEMORY] retrieve_context å¤±è´¥: {e}")
        
        # === ç¬¬äºŒä¼˜å…ˆçº§ï¼šå¯¹è¯å†å²æ£€ç´¢ ===
        if len(memory_texts) < 4:  # å¦‚æœé•¿æœŸè®°å¿†ä¸å¤Ÿï¼Œè¡¥å……å¯¹è¯å†å²
            try:
                recent_conversations = memori.get_conversation_history(limit=15)
                print(f"[MEMORY] å¯¹è¯å†å²æ£€ç´¢åˆ° {len(recent_conversations)} æ¡è®°å½•")
                
                for conv in recent_conversations:
                    user_msg = conv.get('user_input', '')
                    ai_msg = conv.get('ai_output', '')
                    
                    # æ›´æ™ºèƒ½çš„ç›¸å…³æ€§åˆ¤æ–­
                    is_relevant = False
                    if query:
                        query_lower = query.lower()
                        user_lower = user_msg.lower()
                        ai_lower = ai_msg.lower()
                        
                        # ç²¾ç¡®åŒ¹é…æˆ–éƒ¨åˆ†åŒ¹é…
                        if (query_lower in user_lower or query_lower in ai_lower or
                            any(word in user_lower or word in ai_lower 
                                for word in query_lower.split() if len(word) > 1)):
                            is_relevant = True
                    
                    # æ·»åŠ ç›¸å…³è®°å¿†æˆ–å‰å‡ ä¸ªä½œä¸ºå¤‡é€‰
                    if is_relevant or len(memory_texts) < 2:
                        if user_msg and len(user_msg.strip()) > 0:
                            user_msg = user_msg.strip()
                            if len(user_msg) > 120:
                                user_msg = user_msg[:120] + "..."
                            memory_texts.append(f"[å¯¹è¯] ç”¨æˆ·: {user_msg}")
                            
                        if ai_msg and len(ai_msg.strip()) > 0:
                            ai_msg = ai_msg.strip()
                            if len(ai_msg) > 120:
                                ai_msg = ai_msg[:120] + "..."
                            memory_texts.append(f"[å¯¹è¯] AI: {ai_msg}")
                    
                    if len(memory_texts) >= 8:  # é™åˆ¶æ€»æ•°
                        break
                            
            except Exception as e:
                import traceback
                traceback.print_exc()
                print(f"[MEMORY] å¯¹è¯å†å²æ£€ç´¢å¤±è´¥: {e}")
        
        # === ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šç›´æ¥æ•°æ®åº“æœç´¢ ===
        if len(memory_texts) < 2:
            try:
                direct_memories = retrieve_memories_direct_sql(query)
                print(f"[MEMORY] ç›´æ¥æ•°æ®åº“æœç´¢è¿”å› {len(direct_memories)} æ¡è®°å¿†")
                memory_texts.extend([f"[ç›´æ¥] {mem}" for mem in direct_memories[:3]])
            except Exception as e:
                print(f"[MEMORY] ç›´æ¥æ•°æ®åº“æœç´¢å¤±è´¥: {e}")
        
        # === ç»“æœå¤„ç†å’Œæ ¼å¼åŒ– ===
        if memory_texts:
            # æ™ºèƒ½å»é‡å’Œæ’åº
            unique_memories = []
            seen_content = set()
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šé•¿æœŸè®°å¿† > å¯¹è¯å†å² > ç›´æ¥æœç´¢
            priority_order = {'[ESSENTIAL]': 0, '[CONSCIOUS_INFO]': 0, '[CONTEXTUAL]': 1, 
                          '[å¯¹è¯]': 2, '[ç›´æ¥]': 3, '[CONVERSATIONAL]': 4}
            
            def get_priority(memory):
                for prefix, priority in priority_order.items():
                    if memory.startswith(prefix):
                        return priority
                return 5  # é»˜è®¤ä¼˜å…ˆçº§
            
            # æ’åºå¹¶å»é‡
            sorted_memories = sorted(memory_texts, key=get_priority)
            
            for memory in sorted_memories:
                # æå–å†…å®¹è¿›è¡Œå»é‡ï¼ˆå¿½ç•¥å‰ç¼€ï¼‰
                content = memory
                for prefix in priority_order.keys():
                    if memory.startswith(prefix):
                        content = memory[len(prefix):].strip()
                        break
                
                content_lower = content.lower()
                if content_lower not in seen_content and len(content.strip()) > 0:
                    seen_content.add(content_lower)
                    unique_memories.append(memory)
                if len(unique_memories) >= 6:
                    break
            
            print(f"[MEMORY] æœ€ç»ˆè¿”å› {len(unique_memories)} æ¡è®°å¿†")
            return "\n".join([f"{i+1}. {text}" for i, text in enumerate(unique_memories)])
        
        print("[MEMORY] æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
        return ""
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"[MEMORY] è®°å¿†æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return ""


def retrieve_memories_direct_sql(query: str) -> list:
    """ç›´æ¥ä»æ•°æ®åº“æ£€ç´¢è®°å¿†ï¼Œç»•è¿‡ FTS"""
    try:
        import sqlite3
        conn = sqlite3.connect("local_memory.db")
        cursor = conn.cursor()
        
        # åˆ†åˆ«æŸ¥è¯¢çŸ­æœŸå’Œé•¿æœŸè®°å¿†ï¼Œé¿å… UNION çš„ ORDER BY é—®é¢˜
        memory_texts = []
        
        # æœç´¢çŸ­æœŸè®°å¿†
        cursor.execute("""
            SELECT searchable_content, summary, processed_data, created_at
            FROM short_term_memory 
            WHERE user_id = 'default_user' 
            AND (
                searchable_content LIKE ? OR 
                summary LIKE ? OR
                processed_data LIKE ?
            )
            ORDER BY created_at DESC
            LIMIT 5
        """, (f"%{query}%", f"%{query}%", f"%{query}%"))
        
        short_results = cursor.fetchall()
        
        # æœç´¢é•¿æœŸè®°å¿†
        cursor.execute("""
            SELECT searchable_content, summary, processed_data, created_at
            FROM long_term_memory 
            WHERE user_id = 'default_user' 
            AND (
                searchable_content LIKE ? OR 
                summary LIKE ? OR
                processed_data LIKE ?
            )
            ORDER BY created_at DESC
            LIMIT 5
        """, (f"%{query}%", f"%{query}%", f"%{query}%"))
        
        long_results = cursor.fetchall()
        
        conn.close()
        
        # åˆå¹¶ç»“æœï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
        all_results = short_results + long_results
        # ç®€å•æ’åºï¼šçŸ­æœŸè®°å¿†ä¼˜å…ˆï¼Œç„¶åæŒ‰åˆ›å»ºæ—¶é—´
        all_results.sort(key=lambda x: (x[3] if x[3] else ''), reverse=True)
        
        conn.close()
        
        memory_texts = []
        for i, (content, summary, processed_data, created_at) in enumerate(all_results):
            # ä¼˜å…ˆä½¿ç”¨ processed_data ä¸­çš„å†…å®¹
            memory_text = ""
            
            if processed_data:
                try:
                    import json
                    data = json.loads(processed_data)
                    parsed_content = data.get('content', '')
                    if parsed_content:
                        memory_text = parsed_content
                except:
                    pass
            
            # å¦‚æœæ²¡æœ‰è§£æåˆ°å†…å®¹ï¼Œä½¿ç”¨å…¶ä»–å­—æ®µ
            if not memory_text:
                memory_text = content or summary or ""
            
            # æ¸…ç†å’Œé™åˆ¶é•¿åº¦
            if memory_text and len(memory_text.strip()) > 0:
                memory_text = memory_text.strip()
                if len(memory_text) > 200:
                    memory_text = memory_text[:200] + "..."
                memory_texts.append(memory_text)
        
        return memory_texts
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return []


def store_conversation(memori: Memori, user_input: str, ai_response: str):
    """å­˜å‚¨å¯¹è¯åˆ°è®°å¿†ç³»ç»Ÿï¼ˆç¡®ä¿è®°å¿†å¤„ç†å®Œæˆï¼Œæ”¯æŒæ‰€æœ‰æ¨¡å¼ï¼šConsciousã€Autoã€Combinedï¼‰"""
    try:
        # è®°å½•å¯¹è¯
        chat_id = memori.record_conversation(
            user_input=user_input, 
            ai_output=ai_response,
            model=st.session_state.get("model_name", "gemini-2.5-flash")
        )
        
        # å¦‚æœæœ‰ Memory Agentï¼Œéœ€è¦ç¡®ä¿å¼‚æ­¥è®°å¿†å¤„ç†å®Œæˆï¼ˆæ‰€æœ‰æ¨¡å¼éƒ½éœ€è¦ï¼‰
        if memori.memory_agent:
            import time
            
            # ç­‰å¾…å¼‚æ­¥è®°å¿†å¤„ç†å¼€å§‹
            time.sleep(2)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„è®°å¿†ä»»åŠ¡
            max_wait_time = 15  # æœ€å¤šç­‰å¾…15ç§’
            wait_interval = 1   # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
            waited_time = 0
            
            print(f"[MEMORY] ç­‰å¾…è®°å¿†å¤„ç†å®Œæˆ - ID: {chat_id[:8] if chat_id else 'N/A'}...")
            
            while waited_time < max_wait_time:
                try:
                    # æ£€æŸ¥å¯¹è¯å†å²æ˜¯å¦å·²ç»è®°å½•
                    history = memori.get_conversation_history(limit=10)
                    chat_found = any(
                        conv.get('user_input', '') == user_input and 
                        conv.get('ai_output', '') == ai_response 
                        for conv in history
                    )
                    
                    if chat_found:
                        print(f"[MEMORY] å¯¹è¯å†å²å·²è®°å½•")
                        break
                    
                    time.sleep(wait_interval)
                    waited_time += wait_interval
                    
                except Exception as e:
                    print(f"[MEMORY] æ£€æŸ¥å¯¹è¯å†å²æ—¶å‡ºé”™: {e}")
                    time.sleep(wait_interval)
                    waited_time += wait_interval
            
            if waited_time >= max_wait_time:
                print(f"[MEMORY] è­¦å‘Šï¼šè®°å¿†å¤„ç†å¯èƒ½æœªå®Œæˆï¼Œç­‰å¾…è¶…æ—¶")
            else:
                print(f"[MEMORY] è®°å¿†å¤„ç†å®Œæˆï¼Œç­‰å¾…æ—¶é—´: {waited_time}ç§’")
        
        return chat_id
        
    except Exception as e:
        # æ‰“å°é”™è¯¯ä»¥ä¾¿è°ƒè¯•
        print(f"[MEMORY] å­˜å‚¨å¯¹è¯æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        pass


# ============== AI å¯¹è¯ç”Ÿæˆ ==============
def generate_response(user_input: str) -> str:
    """RAG å¯¹è¯æµç¨‹"""
    api_key = st.session_state.api_key
    model = st.session_state.model_name
    persona = st.session_state.persona
    temperature = st.session_state.temperature
    
    memori = get_memori_instance()
    
    # æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆåœ¨ç”Ÿæˆå›å¤å‰ï¼‰
    memory_context = ""
    if memori:
        # æ ¹æ®è®°å¿†æ¨¡å¼é€‰æ‹©ä¸åŒçš„æ£€ç´¢æ–¹å¼
        memori_mode = st.session_state.get("memori_mode", "auto")
        
        if memori_mode == "combined":
            # Combined æ¨¡å¼ï¼šæ˜ç¡®åŒæ—¶ä½¿ç”¨ Conscious å’Œ Auto ä¸¤ç§æ¨¡å¼çš„ä¸Šä¸‹æ–‡
            try:
                context_items = []
                seen_memory_ids = set()
                
                # 1. å§‹ç»ˆè·å– Conscious æ¨¡å¼çš„çŸ­æœŸè®°å¿†ï¼ˆå…³é”®è®°å¿†ï¼‰
                if memori.conscious_ingest:
                    try:
                        # è·å–å…³é”®è®°å¿†ï¼ˆessential conversationsï¼‰
                        essential_conversations = memori.get_essential_conversations(limit=5)
                        print(f"[MEMORY] Combined æ¨¡å¼ - Essential å…³é”®è®°å¿†ï¼š{len(essential_conversations) if essential_conversations else 0} æ¡")
                        
                        for item in essential_conversations:
                            content = ""
                            memory_id = item.get('memory_id', '') if isinstance(item, dict) else None
                            
                            # æå–å†…å®¹
                            if isinstance(item, dict):
                                content = item.get('summary') or item.get('searchable_content')
                                
                                if not content:
                                    processed_data = item.get('processed_data')
                                    if processed_data:
                                        if isinstance(processed_data, str):
                                            try:
                                                import json
                                                processed_data = json.loads(processed_data)
                                            except:
                                                pass
                                        
                                        if isinstance(processed_data, dict):
                                            content = (processed_data.get('content') or 
                                                      processed_data.get('summary') or
                                                      processed_data.get('user_input') or
                                                      processed_data.get('ai_output'))
                            
                            if content and memory_id not in seen_memory_ids:
                                content = str(content).strip()
                                if len(content) > 200:
                                    content = content[:200] + "..."
                                context_items.append(f"[å…³é”®è®°å¿†] {content}")
                                if memory_id:
                                    seen_memory_ids.add(memory_id)
                    except Exception as e:
                        print(f"[MEMORY] Combined æ¨¡å¼ - Essential å…³é”®è®°å¿†è·å–å¤±è´¥: {e}")
                
                # 2. å§‹ç»ˆè·å– Auto Ingest æ¨¡å¼çš„åŠ¨æ€æ£€ç´¢è®°å¿†ï¼ˆä¼˜å…ˆä½¿ç”¨æ™ºèƒ½æœç´¢å¼•æ“ï¼‰
                if memori.auto_ingest:
                    auto_context = []
                    search_method = None
                    
                    # ä¼˜å…ˆå°è¯•ä½¿ç”¨æ™ºèƒ½æœç´¢å¼•æ“ï¼ˆAuto Ingest çš„æ ¸å¿ƒåŠŸèƒ½ï¼‰
                    if memori.search_engine:
                        try:
                            print(f"[MEMORY] Combined æ¨¡å¼ - å°è¯•ä½¿ç”¨æ™ºèƒ½æœç´¢å¼•æ“è¿›è¡Œ Auto Ingest æ£€ç´¢")
                            auto_context = memori.search_engine.execute_search(
                                query=user_input,
                                db_manager=memori.db_manager,
                                user_id=memori.user_id,
                                assistant_id=memori.assistant_id,
                                session_id=memori.session_id,
                                limit=5,
                            )
                            if auto_context:
                                search_method = "æ™ºèƒ½æœç´¢å¼•æ“"
                                print(f"[MEMORY] Combined æ¨¡å¼ - æ™ºèƒ½æœç´¢å¼•æ“è¿”å› {len(auto_context)} æ¡è®°å¿†")
                        except Exception as e:
                            print(f"[MEMORY] Combined æ¨¡å¼ - æ™ºèƒ½æœç´¢å¼•æ“å¤±è´¥: {e}ï¼Œå›é€€åˆ°ç›´æ¥æœç´¢")
                    
                    # å¦‚æœæ™ºèƒ½æœç´¢å¼•æ“å¤±è´¥æˆ–æœªå¯ç”¨ï¼Œä½¿ç”¨ _get_auto_ingest_contextï¼ˆåŒ…å«ç›´æ¥æ•°æ®åº“æœç´¢å’Œå›é€€é€»è¾‘ï¼‰
                    if not auto_context:
                        try:
                            print(f"[MEMORY] Combined æ¨¡å¼ - ä½¿ç”¨ _get_auto_ingest_context è¿›è¡Œæ£€ç´¢")
                            auto_context = memori._get_auto_ingest_context(user_input)
                            if auto_context:
                                # æ£€æŸ¥æ£€ç´¢æ–¹æ³•
                                first_item = auto_context[0] if auto_context else {}
                                search_method = first_item.get('retrieval_method', 'ç›´æ¥æ•°æ®åº“æœç´¢')
                                print(f"[MEMORY] Combined æ¨¡å¼ - {search_method}è¿”å› {len(auto_context)} æ¡è®°å¿†")
                        except Exception as e:
                            import traceback
                            traceback.print_exc()
                            print(f"[MEMORY] Combined æ¨¡å¼ - Auto Ingest ä¸Šä¸‹æ–‡è·å–å¤±è´¥: {e}")
                    
                    # å¤„ç†æ£€ç´¢åˆ°çš„è®°å¿†
                    if auto_context:
                        for item in auto_context[:5]:  # é™åˆ¶ä¸ºå‰5æ¡åŠ¨æ€è®°å¿†
                            content = ""
                            memory_id = item.get('memory_id', '') if isinstance(item, dict) else None
                            memory_type = item.get('memory_type', '') if isinstance(item, dict) else ''
                            
                            # è·³è¿‡å·²æ·»åŠ çš„è®°å¿†ï¼ˆå»é‡ï¼‰
                            if memory_id and memory_id in seen_memory_ids:
                                continue
                            
                            # æå–å†…å®¹
                            if isinstance(item, dict):
                                content = item.get('summary') or item.get('searchable_content')
                                
                                # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•ä» processed_data ä¸­æå–
                                if not content:
                                    processed_data = item.get('processed_data')
                                    if processed_data:
                                        if isinstance(processed_data, str):
                                            try:
                                                import json
                                                processed_data = json.loads(processed_data)
                                            except:
                                                pass
                                        
                                        if isinstance(processed_data, dict):
                                            content = (processed_data.get('content') or 
                                                      processed_data.get('summary') or
                                                      processed_data.get('user_input') or
                                                      processed_data.get('ai_output'))
                            
                            if content:
                                content = str(content).strip()
                                if len(content) > 200:
                                    content = content[:200] + "..."
                                
                                type_label = f"[{memory_type}]" if memory_type else "[åŠ¨æ€]"
                                context_items.append(f"{type_label} {content}")
                                if memory_id:
                                    seen_memory_ids.add(memory_id)
                        
                        print(f"[MEMORY] Combined æ¨¡å¼ - Auto Ingest ä½¿ç”¨ {search_method or 'é»˜è®¤æ–¹æ³•'} æ£€ç´¢åˆ° {len(context_items)} æ¡åŠ¨æ€è®°å¿†")
                
                if context_items:
                    memory_context = "\n".join([f"{i+1}. {item}" for i, item in enumerate(context_items[:8])])
                    print(f"[MEMORY] Combined æ¨¡å¼ - æœ€ç»ˆè¿”å› {len(context_items)} æ¡è®°å¿†ï¼ˆå…³é”®è®°å¿† + Auto Ingest åŠ¨æ€è®°å¿†ï¼‰")
                else:
                    print(f"[MEMORY] Combined æ¨¡å¼ - æœªæ£€ç´¢åˆ°ä»»ä½•è®°å¿†ï¼Œå›é€€åˆ°æ‰‹åŠ¨æ£€ç´¢")
                    memory_context = retrieve_memories(memori, user_input)
                    
            except Exception as e:
                import traceback
                traceback.print_exc()
                print(f"[MEMORY] Combined æ¨¡å¼æ£€ç´¢å¤±è´¥ï¼Œå›é€€åˆ°æ‰‹åŠ¨æ£€ç´¢: {e}")
                memory_context = retrieve_memories(memori, user_input)
        
        elif memori_mode == "auto" and memori.auto_ingest:
            # Auto Ingest æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ Memori çš„è‡ªåŠ¨ä¸Šä¸‹æ–‡æ£€ç´¢
            try:
                # ä½¿ç”¨ _get_auto_ingest_context è·å–é•¿æœŸè®°å¿†ä¸Šä¸‹æ–‡
                auto_context = memori._get_auto_ingest_context(user_input)
                print(f"[MEMORY] Auto Ingest æ£€ç´¢åˆ° {len(auto_context) if auto_context else 0} æ¡è®°å¿†")
                
                if auto_context:
                    # æ ¼å¼åŒ–è‡ªåŠ¨æ£€ç´¢çš„ä¸Šä¸‹æ–‡
                    context_items = []
                    for item in auto_context[:5]:  # é™åˆ¶ä¸ºå‰5æ¡
                        content = ""
                        memory_type = ""
                        
                        # å°è¯•ä»ä¸åŒå­—æ®µè·å–å†…å®¹
                        if isinstance(item, dict):
                            # è·å–è®°å¿†ç±»å‹
                            memory_type = item.get('memory_type', '')
                            
                            # ä¼˜å…ˆä½¿ç”¨ summary æˆ– searchable_content
                            content = item.get('summary') or item.get('searchable_content')
                            
                            # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•ä» processed_data ä¸­æå–
                            if not content:
                                processed_data = item.get('processed_data')
                                if processed_data:
                                    # processed_data å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼ˆJSONï¼‰æˆ–å­—å…¸
                                    if isinstance(processed_data, str):
                                        try:
                                            import json
                                            processed_data = json.loads(processed_data)
                                        except:
                                            pass
                                    
                                    if isinstance(processed_data, dict):
                                        content = (processed_data.get('content') or 
                                                  processed_data.get('summary') or
                                                  processed_data.get('user_input') or
                                                  processed_data.get('ai_output'))
                            
                            # æœ€åå°è¯• content å­—æ®µ
                            if not content:
                                content = item.get('content')
                            
                            # å¦‚æœè¿˜æ˜¯æ²¡å†…å®¹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            if not content:
                                content = str(item.get('processed_data', ''))
                        else:
                            content = str(item)
                        
                        # æ¸…ç†å’Œæ ¼å¼åŒ–å†…å®¹
                        if content and len(str(content).strip()) > 0:
                            content = str(content).strip()
                            if len(content) > 200:
                                content = content[:200] + "..."
                            
                            # æ·»åŠ è®°å¿†ç±»å‹æ ‡è¯†
                            type_label = f"[{memory_type}] " if memory_type else ""
                            context_items.append(f"{type_label}{content}")
                    
                    if context_items:
                        memory_context = "\n".join([f"{i+1}. {item}" for i, item in enumerate(context_items)])
                        print(f"[MEMORY] Auto Ingest æ ¼å¼åŒ–åè¿”å› {len(context_items)} æ¡è®°å¿†")
                    else:
                        print(f"[MEMORY] Auto Ingest æ£€ç´¢åˆ° {len(auto_context)} æ¡è®°å½•ï¼Œä½†æ— æ³•æå–æœ‰æ•ˆå†…å®¹")
                
            except Exception as e:
                import traceback
                traceback.print_exc()
                print(f"[MEMORY] Auto Ingest æ£€ç´¢å¤±è´¥ï¼Œå›é€€åˆ°æ‰‹åŠ¨æ£€ç´¢: {e}")
                # å›é€€åˆ°æ‰‹åŠ¨æ£€ç´¢
                memory_context = retrieve_memories(memori, user_input)
        else:
            # å…¶ä»–æ¨¡å¼ï¼ˆConscious ç­‰ï¼‰ï¼šä½¿ç”¨æ‰‹åŠ¨æ£€ç´¢
            memory_context = retrieve_memories(memori, user_input)
    
    # æ ¼å¼åŒ–è®°å¿†ä¸Šä¸‹æ–‡ï¼šå°†ç”Ÿç¡¬çš„åˆ—è¡¨è½¬æ¢ä¸ºå™è¿°æ€§æ–‡æœ¬
    if memory_context:
        formatted_memory_context = format_memory_context_narrative(memory_context)
        print(f"[MEMORY] è®°å¿†æ ¼å¼åŒ–ï¼šåŸå§‹é•¿åº¦={len(memory_context)}ï¼Œæ ¼å¼åŒ–åé•¿åº¦={len(formatted_memory_context)}")
    else:
        formatted_memory_context = "ï¼ˆæš‚æ— å›å¿†ï¼‰"
    
    # æ„å»ºç³»ç»Ÿæç¤ºè¯
    # å¦‚æœæç¤ºè¯ä¸­åŒ…å« {memory_context} å ä½ç¬¦ï¼Œåˆ™æ›¿æ¢å®ƒï¼›å¦åˆ™è¿½åŠ è®°å¿†ä¸Šä¸‹æ–‡
    if "{memory_context}" in persona:
        # ä½¿ç”¨å ä½ç¬¦æ›¿æ¢æ–¹å¼ï¼ˆä½¿ç”¨æ ¼å¼åŒ–åçš„è®°å¿†ï¼‰
        system_content = persona.replace("{memory_context}", formatted_memory_context)
    else:
        # å…¼å®¹æ—§æ ¼å¼ï¼šè¿½åŠ è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ ¼å¼åŒ–åçš„è®°å¿†ï¼‰
        system_content = persona
        if formatted_memory_context and formatted_memory_context != "ï¼ˆæš‚æ— å›å¿†ï¼‰":
            system_content += f"\n\nã€é•¿æœŸè®°å¿†å‚è€ƒï¼ˆæ¥è‡ªä¹‹å‰çš„å¯¹è¯ï¼‰ã€‘:\n{formatted_memory_context}\n\nè¯·åŸºäºè¿™äº›è®°å¿†ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœè®°å¿†ä¸­æåˆ°ç”¨æˆ·çš„åå­—ã€åå¥½æˆ–å…¶ä»–ä¸ªäººä¿¡æ¯ï¼Œè¯·è®°ä½å¹¶ä½¿ç”¨è¿™äº›ä¿¡æ¯ã€‚"
    
    # ç”Ÿæˆå›å¤
    # ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜çš„LLMå®ä¾‹ï¼ˆå¦‚æœå¯èƒ½ï¼‰
    # æ³¨æ„ï¼šç”±äºapi_keyå’Œtemperatureå¯èƒ½å˜åŒ–ï¼Œè¿™é‡Œæ¯æ¬¡åˆ›å»ºæ–°å®ä¾‹
    # ä½†ChatGoogleGenerativeAIå†…éƒ¨å¯èƒ½æœ‰è¿æ¥æ± ä¼˜åŒ–
    llm = ChatGoogleGenerativeAI(
        model=model,
        google_api_key=api_key,
        temperature=temperature,
        convert_system_message_to_human=True,
    )
    
    messages = [
        SystemMessage(content=system_content),
        HumanMessage(content=user_input),
    ]
    
    response = llm.invoke(messages)
    ai_response = response.content
    
    # å­˜å‚¨å¯¹è¯åˆ°è®°å¿†ç³»ç»Ÿï¼ˆç”Ÿæˆå›å¤åï¼‰
    if memori:
        store_conversation(memori, user_input, ai_response)
    
    return ai_response


# ============== è®¾ç½®é¢æ¿ï¼ˆä¾§è¾¹æ å±•å¼€å™¨ï¼‰ ==============
def render_settings_panel():
    """åœ¨ä¾§è¾¹æ æ¸²æŸ“è®¾ç½®é¢æ¿"""
    with st.expander(f"âš™ï¸ {t('settings')}", expanded=False):
        st.markdown(f"**{t('api_config')}**")
        
        api_key = st.text_input(
            t("google_api_key"),
            value=st.session_state.api_key,
            type="password",
            placeholder=t("api_key_placeholder"),
            help=t("api_key_help"),
            key="sidebar_api_key"
        )
        
        model_name = st.text_input(
            t("model_name"),
            value=st.session_state.model_name,
            placeholder="gemini-2.5-flash",
            help=t("model_help"),
            key="sidebar_model"
        )
        
        temperature = st.slider(
            t("temperature"),
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.temperature,
            step=0.1,
            help=t("temperature_help"),
            key="sidebar_temp"
        )
        
        st.markdown("---")
        st.markdown(f"**{t('memory_mode')}**")
        
        memori_mode = st.selectbox(
            t("memory_mode"),
            options=["conscious", "auto", "combined"],
            index=["conscious", "auto", "combined"].index(st.session_state.get("memori_mode", "auto")),
            format_func=lambda x: {
                "conscious": t("memory_mode_conscious"),
                "auto": t("memory_mode_auto"),
                "combined": t("memory_mode_combined")
            }[x],
            help=t("memory_mode_help"),
            key="sidebar_memori_mode"
        )
        
        st.markdown("---")
        st.markdown(f"**{t('persona_title')}**")
        
        persona = st.text_area(
            t("persona_label"),
            value=st.session_state.persona,
            height=100,
            help=t("persona_help"),
            key="sidebar_persona"
        )
        
        st.markdown("---")
        st.markdown(f"**{t('avatar_settings')}**")
        
        # ç”¨æˆ·å¤´åƒä¸Šä¼ 
        col_upload_user, col_preview_user = st.columns([2, 1])
        with col_upload_user:
            uploaded_user_avatar = st.file_uploader(
                t("user_avatar"),
                type=['png', 'jpg', 'jpeg', 'gif', 'webp'],
                help=t("avatar_help"),
                key="upload_user_avatar"
            )
            
            if uploaded_user_avatar is not None:
                # å¤„ç†ä¸Šä¼ çš„å¤´åƒ
                avatar_data = crop_circle_image(uploaded_user_avatar.read(), size=64)
                if avatar_data:
                    st.session_state.user_avatar = avatar_data
                    # ç«‹å³ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
                    save_config()
                    st.success("âœ… ç”¨æˆ·å¤´åƒå·²æ›´æ–°")
                    st.rerun()
            
            # æ˜¾ç¤ºå½“å‰ç”¨æˆ·å¤´åƒé¢„è§ˆ
            if st.session_state.get("user_avatar"):
                if st.button(f"âŒ {t('remove_avatar')}", key="remove_user_avatar", use_container_width=True):
                    st.session_state.user_avatar = ""
                    save_config()
                    st.rerun()
        
        with col_preview_user:
            if st.session_state.get("user_avatar"):
                st.markdown(f"""
                <div style="display: flex; justify-content: center; align-items: center; height: 80px;">
                    <img src="{st.session_state.user_avatar}" 
                         style="width: 64px; height: 64px; border-radius: 50%; object-fit: cover; border: 2px solid #667eea;" />
                </div>
                """, unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div style="display: flex; justify-content: center; align-items: center; height: 80px;">
                    <div style="width: 64px; height: 64px; border-radius: 50%; background: #e3f2fd; display: flex; align-items: center; justify-content: center; font-size: 32px;">ğŸ‘¤</div>
                </div>
                """, unsafe_allow_html=True)
        
        # åŠ©æ‰‹å¤´åƒä¸Šä¼ 
        col_upload_assistant, col_preview_assistant = st.columns([2, 1])
        with col_upload_assistant:
            uploaded_assistant_avatar = st.file_uploader(
                t("assistant_avatar"),
                type=['png', 'jpg', 'jpeg', 'gif', 'webp'],
                help=t("avatar_help"),
                key="upload_assistant_avatar"
            )
            
            if uploaded_assistant_avatar is not None:
                # å¤„ç†ä¸Šä¼ çš„å¤´åƒ
                avatar_data = crop_circle_image(uploaded_assistant_avatar.read(), size=64)
                if avatar_data:
                    st.session_state.assistant_avatar = avatar_data
                    # ç«‹å³ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
                    save_config()
                    st.success("âœ… åŠ©æ‰‹å¤´åƒå·²æ›´æ–°")
                    st.rerun()
            
            # æ˜¾ç¤ºå½“å‰åŠ©æ‰‹å¤´åƒé¢„è§ˆ
            if st.session_state.get("assistant_avatar"):
                if st.button(f"âŒ {t('remove_avatar')}", key="remove_assistant_avatar", use_container_width=True):
                    st.session_state.assistant_avatar = ""
                    save_config()
                    st.rerun()
        
        with col_preview_assistant:
            if st.session_state.get("assistant_avatar"):
                st.markdown(f"""
                <div style="display: flex; justify-content: center; align-items: center; height: 80px;">
                    <img src="{st.session_state.assistant_avatar}" 
                         style="width: 64px; height: 64px; border-radius: 50%; object-fit: cover; border: 2px solid #667eea;" />
                </div>
                """, unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div style="display: flex; justify-content: center; align-items: center; height: 80px;">
                    <div style="width: 64px; height: 64px; border-radius: 50%; background: #f3e5f5; display: flex; align-items: center; justify-content: center; font-size: 32px;">ğŸ¤–</div>
                </div>
                """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # ä¿å­˜æŒ‰é’®
        if st.button(t("save_settings"), type="primary", use_container_width=True, key="save_btn"):
            st.session_state.api_key = api_key
            st.session_state.model_name = model_name
            st.session_state.temperature = temperature
            st.session_state.persona = persona
            st.session_state.memori_mode = memori_mode
            
            # å¦‚æœ API key æˆ–è®°å¿†æ¨¡å¼æ”¹å˜ï¼Œæ¸…é™¤ç¼“å­˜
            if (api_key != st.session_state.get("_last_api_key", "") or 
                memori_mode != st.session_state.get("_last_memori_mode", "")):
                st.cache_resource.clear()
                st.session_state._last_api_key = api_key
                st.session_state._last_memori_mode = memori_mode
            
            if save_config():
                st.success(t("settings_saved"))
                st.rerun()
        
        # çŠ¶æ€æŒ‡ç¤º
        if st.session_state.api_key:
            st.success(t("api_key_configured"))
        else:
            st.warning(t("api_key_required"))


# ============== è®°å¿†é¢æ¿ï¼ˆä¾§è¾¹æ å±•å¼€å™¨ï¼‰ ==============
def render_memory_panel():
    """åœ¨ä¾§è¾¹æ æ¸²æŸ“è®°å¿†é¢æ¿"""
    with st.expander(f"ğŸ§  {t('memory_status')}", expanded=False):
        db_file = Path("local_memory.db")
        
        if db_file.exists():
            size_kb = db_file.stat().st_size / 1024
            st.markdown(f"""
            <div class="memory-card">
                <span class="status-indicator status-active"></span>
                <strong>{t('memory_connected')}</strong><br>
                ğŸ“ {t('memory_path')}: local_memory.db<br>
                ğŸ’¾ {t('memory_size')}: {size_kb:.1f} KB
            </div>
            """, unsafe_allow_html=True)
            
            memori = get_memori_instance()
            if memori:
                try:
                    # è·å–æœ€è¿‘çš„å¯¹è¯å†å²
                    recent_conversations = memori.get_conversation_history(limit=3)
                    if recent_conversations:
                        st.markdown(f"{t('recent_count')}: **{len(recent_conversations)}**")
                        for i, conv in enumerate(recent_conversations, 1):
                            user_msg = conv.get('user_input', '')
                            if user_msg:
                                display_content = user_msg[:60] + "..." if len(user_msg) > 60 else user_msg
                                st.caption(f"#{i}: {display_content}")
                    else:
                        st.info(t("no_memories"))
                except Exception as e:
                    import traceback
                    traceback.print_exc()
                    st.info(t("memory_ready"))
            else:
                st.info(t("config_api_first"))
        else:
            st.markdown(f"""
            <div class="memory-card">
                <span class="status-indicator status-inactive"></span>
                <strong>{t('memory_not_created')}</strong><br>
                {t('memory_will_create')}
            </div>
            """, unsafe_allow_html=True)
            st.info(t("memory_tip"))


# ============== ä¾§è¾¹æ  ==============
def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    with st.sidebar:
        # ===== è®¾ç½®é¢æ¿ =====
        render_settings_panel()
        
        # ===== è®°å¿†é¢æ¿ =====
        render_memory_panel()
        
        st.markdown("<div style='height: 10px;'></div>", unsafe_allow_html=True)
        
        # ===== æ–°å¯¹è¯æŒ‰é’® =====
        if st.button(t("new_chat"), use_container_width=True, type="primary"):
            new_id = create_new_conversation()
            st.session_state.current_conversation_id = new_id
            st.session_state.editing_message_index = None
            st.rerun()
        
        st.markdown("<div style='height: 8px;'></div>", unsafe_allow_html=True)
        st.markdown(f'<p class="sidebar-section-title">{t("chat_history")}</p>', unsafe_allow_html=True)
        
        conversations = list(st.session_state.conversations.values())
        conversations.sort(key=lambda x: x["created_at"], reverse=True)
        
        for conv in conversations:
            conv_id = conv["id"]
            is_active = conv_id == st.session_state.current_conversation_id
            
            col1, col2 = st.columns([5, 1])
            
            with col1:
                btn_type = "primary" if is_active else "secondary"
                if st.button(
                    f"{'ğŸ“Œ ' if is_active else 'ğŸ’­ '}{conv['title'][:12]}",
                    key=f"conv_{conv_id}",
                    use_container_width=True,
                    type=btn_type
                ):
                    st.session_state.current_conversation_id = conv_id
                    st.session_state.editing_message_index = None
                    save_conversations()  # ä¿å­˜å¯¹è¯å†å²
                    st.rerun()
            
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"del_{conv_id}", help=t("delete")):
                    if len(st.session_state.conversations) > 1:
                        del st.session_state.conversations[conv_id]
                        if conv_id == st.session_state.current_conversation_id:
                            remaining = list(st.session_state.conversations.keys())
                            st.session_state.current_conversation_id = remaining[0]
                        save_conversations()  # ä¿å­˜å¯¹è¯å†å²
                        st.rerun()
            
            st.caption(f"ğŸ• {conv['created_at']}")


# ============== é¡¶éƒ¨å¯¼èˆªæ  ==============
def render_topbar():
    """æ¸²æŸ“é¡¶éƒ¨å¯¼èˆªæ  - ä¾§è¾¹æ åˆ‡æ¢æŒ‰é’®å’Œè¯­è¨€åˆ‡æ¢æŒ‰é’®"""
    lang = st.session_state.get("language", "zh")
    btn_text = "EN" if lang == "zh" else "ä¸­"
    btn_tooltip = "Switch to English" if lang == "zh" else "åˆ‡æ¢åˆ°ä¸­æ–‡"
    
    st.markdown(f"""
    <style>
        .topbar-buttons {{
            position: fixed;
            top: 14px;
            right: 20px;
            z-index: 1000;
            display: flex;
            gap: 10px;
            align-items: center;
        }}
        .lang-switch-btn {{
            background: white;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 6px 16px;
            font-size: 14px;
            font-weight: 500;
            cursor: pointer;
            color: #333;
            transition: all 0.2s;
        }}
        .lang-switch-btn:hover {{
            background: #f5f5f5;
            border-color: #667eea;
            color: #667eea;
        }}
    </style>
    """, unsafe_allow_html=True)
    
    # ä½¿ç”¨ Streamlit åŸç”ŸæŒ‰é’®ï¼ˆæ”¾åœ¨å³ä¾§åˆ—ä¸­ï¼‰
    _, col_lang = st.columns([12, 1])
    with col_lang:
        if st.button(btn_text, key="lang_btn", help=btn_tooltip):
            st.session_state.language = "en" if lang == "zh" else "zh"
            save_config()
            st.rerun()


# ============== ä¸»èŠå¤©åŒº ==============
def render_chat():
    """æ¸²æŸ“ä¸»èŠå¤©ç•Œé¢"""
    # å¤„ç†å¾…æ›´æ–°çš„æ ‡é¢˜ï¼ˆå¼‚æ­¥ç”Ÿæˆå®Œæˆåçš„æ›´æ–°ï¼‰
    if "_pending_title_updates" in st.session_state:
        for conv_id, title in st.session_state["_pending_title_updates"].items():
            if conv_id in st.session_state.conversations:
                st.session_state.conversations[conv_id]["title"] = title
                st.session_state.conversations[conv_id]["title_generated"] = True
        del st.session_state["_pending_title_updates"]
        save_conversations()  # ä¿å­˜æ ‡é¢˜æ›´æ–°
    
    render_topbar()
    
    st.markdown(f'<h1 class="main-header">{t("main_title")}</h1>', unsafe_allow_html=True)
    st.markdown(f'<p class="sub-header">{t("sub_title")}</p>', unsafe_allow_html=True)
    
    messages = get_current_messages()
    last_user_idx = get_last_user_message_index()
    
    messages_container = st.container()
    
    with messages_container:
        for idx, message in enumerate(messages):
            role = message["role"]
            content = message["content"]
            is_last_user = (role == "user" and idx == last_user_idx)
            
            if role == "user":
                col1, col2, col3 = st.columns([1, 6, 1])
                with col2:
                    if st.session_state.editing_message_index == idx:
                        edit_text = st.text_area(
                            t("edit_message"),
                            value=content,
                            key=f"edit_area_{idx}",
                            height=100,
                            label_visibility="collapsed"
                        )
                        col_save, col_cancel = st.columns(2)
                        with col_save:
                            if st.button(t("send"), key=f"save_{idx}", use_container_width=True):
                                if edit_text.strip():
                                    update_last_user_message(edit_text.strip())
                                    st.session_state.editing_message_index = None
                                    response = generate_response(edit_text.strip())
                                    add_message_to_current("assistant", response)
                                    conv_id = st.session_state.current_conversation_id
                                    # ä¼˜åŒ–ï¼šå¼‚æ­¥ç”Ÿæˆæ ‡é¢˜ï¼Œä¸é˜»å¡å“åº”
                                    if not st.session_state.conversations[conv_id].get("title_generated", False):
                                        def _generate_title_edit():
                                            try:
                                                new_title = generate_conversation_title(get_current_messages())
                                                if "_pending_title_updates" not in st.session_state:
                                                    st.session_state["_pending_title_updates"] = {}
                                                st.session_state["_pending_title_updates"][conv_id] = new_title
                                            except Exception:
                                                pass
                                        thread = threading.Thread(target=_generate_title_edit, daemon=True)
                                        thread.start()
                                        st.session_state.conversations[conv_id]["title_generated"] = True
                                    st.rerun()
                        with col_cancel:
                            if st.button(t("cancel"), key=f"cancel_{idx}", use_container_width=True):
                                st.session_state.editing_message_index = None
                                st.rerun()
                    else:
                        user_avatar_html = get_avatar_html("user", size=36)
                        st.markdown(f"""
                        <div class="message-row user">
                            <div class="message-wrapper">
                                <div class="message-bubble user">{content}</div>
                            </div>
                            <div class="avatar user">{user_avatar_html}</div>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        if is_last_user:
                            _, edit_col = st.columns([9.5, 0.5])
                            with edit_col:
                                if st.button("âœï¸", key=f"edit_btn_{idx}", help=t("edit_resend"), use_container_width=True):
                                    st.session_state.editing_message_index = idx
                                    st.rerun()
            else:
                col1, col2, col3 = st.columns([1, 6, 1])
                with col2:
                    assistant_avatar_html = get_avatar_html("assistant", size=36)
                    st.markdown(f"""
                    <div class="message-row assistant">
                        <div class="avatar assistant">{assistant_avatar_html}</div>
                        <div class="message-wrapper assistant">
                            <div class="message-bubble assistant">{content}</div>
                        </div>
                    </div>
                    """, unsafe_allow_html=True)
    
    st.markdown("<div style='height: 20px;'></div>", unsafe_allow_html=True)
    
    if prompt := st.chat_input(t("input_placeholder"), key="chat_input"):
        if not st.session_state.api_key:
            st.warning(t("api_key_warning"))
            return
        
        st.session_state.editing_message_index = None
        
        # æ¸…é™¤æ‰€æœ‰ç”Ÿæˆç›¸å…³çš„æ ‡å¿—ï¼Œç¡®ä¿æ–°æ¶ˆæ¯å¯ä»¥æ­£å¸¸å¤„ç†
        if "_last_processed_user_idx" in st.session_state:
            del st.session_state["_last_processed_user_idx"]
        if "_generating" in st.session_state:
            del st.session_state["_generating"]
        if "_pending_msg" in st.session_state:
            del st.session_state["_pending_msg"]
        
        add_message_to_current("user", prompt)
        st.rerun()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¾…ç”Ÿæˆå›å¤çš„ç”¨æˆ·æ¶ˆæ¯
    messages = get_current_messages()
    
    # ç®€å•é€»è¾‘ï¼šå¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æ˜¯userï¼Œå°±ç”Ÿæˆå›å¤ï¼ˆå› ä¸ºæ¶ˆæ¯æ˜¯æŒ‰é¡ºåºæ·»åŠ çš„ï¼‰
    if messages and messages[-1]["role"] == "user":
        # ä½¿ç”¨æ¶ˆæ¯ç´¢å¼•æ¥ç¡®ä¿æ¯æ¡æ¶ˆæ¯åªå¤„ç†ä¸€æ¬¡
        last_user_idx = len(messages) - 1
        last_processed_idx = st.session_state.get("_last_processed_user_idx", -1)
        
        # åªæœ‰å½“è¿™æ¡æ¶ˆæ¯è¿˜æ²¡æœ‰è¢«å¤„ç†è¿‡æ—¶æ‰ç”Ÿæˆå›å¤
        if last_user_idx != last_processed_idx:
            # æ ‡è®°è¿™æ¡æ¶ˆæ¯æ­£åœ¨å¤„ç†
            st.session_state["_last_processed_user_idx"] = last_user_idx
            
            user_msg = messages[-1]["content"]
            
            # æ˜¾ç¤ºæ€è€ƒçŠ¶æ€
            col1, col2, col3 = st.columns([1, 6, 1])
            with col2:
                thinking_placeholder = st.empty()
                with thinking_placeholder:
                    st.info(f"ğŸ¤” {t('thinking')}...")
            
            # ç”ŸæˆAIå›å¤
            try:
                response = generate_response(user_msg)
                add_message_to_current("assistant", response)
                
                # æ¸…é™¤æ€è€ƒçŠ¶æ€
                thinking_placeholder.empty()
                
                conv_id = st.session_state.current_conversation_id
                conv = st.session_state.conversations.get(conv_id, {})
                # ä¼˜åŒ–ï¼šå¼‚æ­¥ç”Ÿæˆæ ‡é¢˜ï¼Œä¸é˜»å¡å“åº”
                if not conv.get("title_generated", False) and len(get_current_messages()) >= 2:
                    def _generate_title():
                        """åå°ç”Ÿæˆæ ‡é¢˜"""
                        try:
                            new_title = generate_conversation_title(get_current_messages())
                            # ä½¿ç”¨session_stateæ ‡è®°éœ€è¦æ›´æ–°æ ‡é¢˜
                            if "_pending_title_updates" not in st.session_state:
                                st.session_state["_pending_title_updates"] = {}
                            st.session_state["_pending_title_updates"][conv_id] = new_title
                        except Exception:
                            pass
                    
                    # åœ¨åå°çº¿ç¨‹æ‰§è¡Œæ ‡é¢˜ç”Ÿæˆ
                    thread = threading.Thread(target=_generate_title, daemon=True)
                    thread.start()
                    
                    # æ ‡è®°æ ‡é¢˜æ­£åœ¨ç”Ÿæˆï¼Œé¿å…é‡å¤ç”Ÿæˆ
                    st.session_state.conversations[conv_id]["title_generated"] = True
                
                # æ¸…é™¤å¤„ç†æ ‡è®°ï¼Œå‡†å¤‡å¤„ç†ä¸‹ä¸€æ¡æ¶ˆæ¯
                if "_last_processed_user_idx" in st.session_state:
                    del st.session_state["_last_processed_user_idx"]
                st.rerun()
                
            except Exception as e:
                thinking_placeholder.empty()
                if "_last_processed_user_idx" in st.session_state:
                    del st.session_state["_last_processed_user_idx"]
                st.error(f"{t('error_generating')}: {str(e)}")
                st.rerun()


# ============== ä¸»ç¨‹åºå…¥å£ ==============
def main():
    """ä¸»ç¨‹åºå…¥å£"""
    init_session_state()
    render_sidebar()
    render_chat()


if __name__ == "__main__":
    main()
